---
title: "CS341 - Algorithms"
url: "courses/CS341/"
summary: The study of efficient algorithms and effective algorithm design techniques
ShowToc: true
math: true
---

> **Instructor:** Eric Schost \
> **Lectures:** T/Th at 8:30am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 5 $\times$ 8% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 40% \
> \
> **Course Description:** \
> The study of efficient algorithms and effective algorithm design techniques. Program design with emphasis on pragmatic and mathematical aspects of program efficiency. Topics include divide and conquer algorithms, recurrences, greedy algorithms, dynamic programming, graph search and backtrack, problems without algorithms, NP-completeness and its implications.

<br/>

## Introduction

Building upon our CS240 knowledge of data structures and Big-O notation, as well as prior math, we will learn many algorithms (divide-and-conquer, BFS, DFS, greedy, dynamic programming, flows and cuts) and their pseudocode, correctness, and runtime. We will also briefly discuss NP-completeness and reductions.

### Prerequisite Knowledge

#### Runtime Definitions
Recall that the input (problem instance) of an algorithm is parameterized by $n$ called the size. We then had the following brief definitions:

$T(I)$ - $\text{Runtime on input } I$

$T(n)$ - $\max_{I \text{ of size } n} T(I)$

$T_{avg}(n) - \frac{\sum_{I \text{ of size } n} T(I)}{\text{number of inputs of size } I}$

#### Asymptotic Notation

Consider two functions $f(n)$, $g(n)$ with values in $\mathbb{R}_{>0}$. Then, we have:
* $f(n) \in O(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \leq Cg(n)$
* $f(n) \in \Omega(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \geq Cg(n)$
* $f(n) \in \Theta(g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$
* $f(n) \in o(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) < Cg(n)$
* $f(n) \in \omega(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) > Cg(n)$

In the case of graphs and matrices, our algorithms may contain two parameters $n$ and $m$. We need to redefine our asymptotic notation to accommodate two parameters. For big-$O$ notation, we have the following definition:

$f(n, m) \in O(g(n, m))$ if $\exists C, n_0, m_0$ such that for $n \geq n_0$ or $m \geq m_0$, we have $f(n, m) \leq Cg(n, m)$

#### Examples of Asymptotic Analysis
Here are some useful examples of asymptotic analysis that will be standard for the course going forwards:
* $n^k + c_{k-1}n^{k-1} + ... + c_0 \in \Theta(n^k)$
* $n^{O(1)}$ means (at most) polynomial in $n$
* $n \log(n) \in O(n^2), \Omega(n)$
* $2^{n-1} \in \Omega(2^n)$
* $(n-1)! \in o(n!)$ and NOT $\Theta(n!)$

Here are some practical notes for dealing with asymptotic analysis:
1. Big-$O$ is only an upper bound, and we should aim to give $\Theta$'s if possible.
2. Big-Anything hides constants and only states that one algorithm will _eventually_ beat another algorithm (ie. $\Theta(n^2)$ could be faster than $\Theta(n)$ for small enough input)
3. We will use a simplified model, focusing purely on "operations"

#### Computational Model (word RAM)

Memory locations contain integer words of $b$ bits each, and $b \geq \log(n)$ for input size $n$.

Random Access Memory can access any memory location at unit cost, and all basic operations on words (loop counters, array indices, etc.) have unit costs. Unless otherwise implemented, we will always assume that these basic operations are constant time.

It is possible to perform run-time analysis in terms of integer words using the word RAM model, though we don't have to worry about this.

<br/>

## Divide and Conquer Algorithms

### Recurrences

Let's investigate Merge Sort, which uses divide and conquer to sort an array. With this sorting algorithm, we split the array into halves, sort those halves, then merge. Observe that merging requires at most $n - 1$ comparisons and can be done in-place using the two pointers strategy. Merge sort follows this recurrence relation:

$$ t(1) = 0, \hspace{0.4cm} t(n) = 2t\left(\frac{n}{2}\right) + n - 1$$

The $n - 1$ is not very convenient to analyze, so let's instead define:

$$ T(1) = 0, \hspace{0.4cm} T(n) = 2T\left(\frac{n}{2}\right) + n$$

and observe that $t(n) \leq T(n)$ by induction.

Let's unroll the recurrence relation:

$$
\begin{aligned}
    T(n) &= \color{red}{2} \color{black}T\left(\frac{n}{\color{red}{2} \color{black}}\right) + \color{red}{1} \color{black}n \\\\
         &= 2\left(2T\left(\frac{n}{4}\right) + \frac{n}{2}\right) + n \\\\
         &= \color{red}{4} \color{black}T\left(\frac{n}{\color{red}{4} \color{black}}\right) + \color{red}{2} \color{black}n \\\\
         &= \color{red}{8} \color{black}T\left(\frac{n}{\color{red}{8} \color{black}}\right) + \color{red}{3} \color{black}n \\\\
         &= ... \\\\
         &= \color{red}{n} \color{black}T\left(\frac{n}{\color{red}{n} \color{black}}\right) + \color{red}{\log_{2}(n)} \color{black} n \\\\
         &= nT(1) + n\log_{2}(n) \\\\
         &= n\log_{2}(n) \\\\
\end{aligned}
$$

Thus, we can see that merge sort is $O(n \log_{2}(n))$. If you look at the recursion tree for Merge Sort below, we get the same result:

![Recursion-Tree](/recursion-tree-example.png)

This recursion tree demonstrates how the problems are split up until we get $\frac{n}{2^k} = 1$ and how we add up the work on each level. The tree here has height $\log_2(n)$ and on each level, we perform a $O(n)$ operation, so the total work done is $O(\log_2(n))$.

Note that with Merge Sort, this is actually a tight bound, so Merge Sort has $\Theta(n\log_{2}(n))$ time complexity. The question then becomes, how do we handle more complicated recurrence relations?

### Master Theorem

Consider two integers $a, b \geq 1$, a real number $c \geq 0$, and a function $T(n)$ such that:

$$ T(1) = C, \hspace{0.4cm} T(n) = aT\left(\frac{n}{b}\right) + \Theta(n^c)$$

where $n$ is a power of $b$. Observe that:
* $b$ is the factor by which we reduce the problem size
* $a$ is the number of recursive calls
* $\Theta(n^c)$ is the cost to prepare the recursive calls and combine their results

A simplified version of the master theorem states that:

$$T(n) \in \begin{cases}
    \Theta(n^c) & \text{if } c > \log_{b}(a) \hspace{0.6cm} \text{(root-heavy)} \\\\
    \Theta(n^c \log(n)) & \text{if } c = \log_{b}(a) \hspace{0.6cm} \text{(balanced)} \\\\
    \Theta(n^{\log_{b}(a)}) & \text{if } c < \log_{b}(a) \hspace{0.6cm} \text{(leaf-heavy)}
    \end{cases} $$

A cleaner version of this theorem has $b^k$ in replacement of $n$. We can prove that $n = b^k$ using recursion unrolling, logarithm rules, and case analysis. The proof has been omitted.

If we cannot apply the Master Theorem to a chosen recurrence, then we need to either use a recursion tree or unroll the recurrence. To use the recursion tree argument:

1. Draw the tree based on the recurrence given - the tree's leaves should all be $T(1)$
2. For each level of the tree, identify the size of the subproblems, number of nodes, cost/node, and total cost. It is helpful to identify common patterns in the numbers like shown with the Merge Sort example
3. Sum the costs of all levels of the recursion tree and determine the order

### Divide and Conquer

To solve a problem in size $n$ using the **Divide and Conquer** strategy, we:

1. **Divide**: Break the input into smaller problems, ideally into size $\frac{n}{b}$
2. **Conquer**: Solve these subproblems recursively
3. **Recombine**: Deduce the solution of the main problem from the subproblems

For the remainder of this section, we will see how we can solve some problems using the Divide and Conquer approach.

#### Karatsuba's Algorithm

#### Matrix Multiplication

#### Maximum Subarray

#### Three Sum

#### Counting Inversions

#### Finding the Median


