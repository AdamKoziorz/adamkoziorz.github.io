---
title: "CS341 - Algorithms"
url: "courses/CS341/"
summary: The study of efficient algorithms and effective algorithm design techniques
ShowToc: true
math: true
---

> **Instructor:** Eric Schost \
> **Lectures:** T/Th at 8:30am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 5 $\times$ 8% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 40% \
> \
> **Course Description:** \
> The study of efficient algorithms and effective algorithm design techniques. Program design with emphasis on pragmatic and mathematical aspects of program efficiency. Topics include divide and conquer algorithms, recurrences, greedy algorithms, dynamic programming, graph search and backtrack, problems without algorithms, NP-completeness and its implications.

<br/>

# Introduction

Building upon our CS240 knowledge of data structures and Big-O notation, as well as prior math, we will learn many algorithms (divide-and-conquer, BFS, DFS, greedy, dynamic programming, flows and cuts) and their pseudocode, correctness, and runtime. We will also briefly discuss NP-completeness and reductions.

## Prerequisite Knowledge

### Runtime Definitions
Recall that the input (problem instance) of an algorithm is parameterized by $n$ called the size. We then had the following brief definitions:

$T(I)$ - $\text{Runtime on input } I$

$T(n)$ - $\max_{I \text{ of size } n} T(I)$

$T_{avg}(n) - \frac{\sum_{I \text{ of size } n} T(I)}{\text{number of inputs of size } I}$

### Asymptotic Notation

Consider two functions $f(n)$, $g(n)$ with values in $\mathbb{R}_{>0}$. Then, we have:
* $f(n) \in O(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \leq Cg(n)$
* $f(n) \in \Omega(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \geq Cg(n)$
* $f(n) \in \Theta(g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$
* $f(n) \in o(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) < Cg(n)$
* $f(n) \in \omega(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) > Cg(n)$

In the case of graphs and matrices, our algorithms may contain two parameters $n$ and $m$. We need to redefine our asymptotic notation to accommodate two parameters. For big-$O$ notation, we have the following definition:

$f(n, m) \in O(g(n, m))$ if $\exists C, n_0, m_0$ such that for $n \geq n_0$ or $m \geq m_0$, we have $f(n, m) \leq Cg(n, m)$

### Examples of Asymptotic Analysis
Here are some useful examples of asymptotic analysis that will be standard for the course going forwards:
* $n^k + c_{k-1}n^{k-1} + ... + c_0 \in \Theta(n^k)$
* $n^{O(1)}$ means (at most) polynomial in $n$
* $n \log(n) \in O(n^2), \Omega(n)$
* $2^{n-1} \in \Omega(2^n)$
* $(n-1)! \in o(n!)$ and NOT $\Theta(n!)$

Here are some practical notes for dealing with asymptotic analysis:
1. Big-$O$ is only an upper bound, and we should aim to give $\Theta$'s if possible.
2. Big-Anything hides constants and only states that one algorithm will _eventually_ beat another algorithm (ie. $\Theta(n^2)$ could be faster than $\Theta(n)$ for small enough input)
3. We will use a simplified model, focusing purely on "operations"

### Computational Model (word RAM)

Memory locations contain integer words of $b$ bits each, and $b \geq \log(n)$ for input size $n$.

Random Access Memory can access any memory location at unit cost, and all basic operations on words (loop counters, array indices, etc.) have unit costs. Unless otherwise implemented, we will always assume that these basic operations are constant time.

It is possible to perform run-time analysis in terms of integer words using the word RAM model, though we don't have to worry about this.

<br/>

# Divide and Conquer Algorithms

## Recurrences

Let's investigate Merge Sort, which uses divide and conquer to sort an array. With this sorting algorithm, we split the array into halves, sort those halves, then merge. Observe that merging requires at most $n - 1$ comparisons and can be done in-place using the two pointers strategy. Merge sort follows this recurrence relation:

$$ t(1) = 0, \hspace{0.4cm} t(n) = 2t\left(\frac{n}{2}\right) + n - 1$$

The $n - 1$ is not very convenient to analyze, so let's instead define:

$$ T(1) = 0, \hspace{0.4cm} T(n) = 2T\left(\frac{n}{2}\right) + n$$

and observe that $t(n) \leq T(n)$ by induction.

Let's unroll the recurrence relation:

$$
\begin{aligned}
    T(n) &= \color{red}{2} \color{black}T\left(\frac{n}{\color{red}{2} \color{black}}\right) + \color{red}{1} \color{black}n \\\\
         &= 2\left(2T\left(\frac{n}{4}\right) + \frac{n}{2}\right) + n \\\\
         &= \color{red}{4} \color{black}T\left(\frac{n}{\color{red}{4} \color{black}}\right) + \color{red}{2} \color{black}n \\\\
         &= \color{red}{8} \color{black}T\left(\frac{n}{\color{red}{8} \color{black}}\right) + \color{red}{3} \color{black}n \\\\
         &= ... \\\\
         &= \color{red}{n} \color{black}T\left(\frac{n}{\color{red}{n} \color{black}}\right) + \color{red}{\log_{2}(n)} \color{black} n \\\\
         &= nT(1) + n\log_{2}(n) \\\\
         &= n\log_{2}(n) \\\\
\end{aligned}
$$

Thus, we can see that merge sort is $O(n \log_{2}(n))$. If you look at the recursion tree for Merge Sort below, we get the same result:

![Recursion-Tree](/recursion-tree-example.png)

This recursion tree demonstrates how the problems are split up until we get $\frac{n}{2^k} = 1$ and how we add up the work on each level. The tree here has height $\log_2(n)$ and on each level, we perform a $O(n)$ operation, so the total work done is $O(\log_2(n))$.

Note that with Merge Sort, this is actually a tight bound, so Merge Sort has $\Theta(n\log_{2}(n))$ time complexity. The question then becomes, how do we handle more complicated recurrence relations?

## Master Theorem

Consider two integers $a, b \geq 1$, a real number $c \geq 0$, and a function $T(n)$ such that:

$$ T(1) = C, \hspace{0.4cm} T(n) = aT\left(\frac{n}{b}\right) + \Theta(n^c)$$

where $n$ is a power of $b$. Observe that:
* $b$ is the factor by which we reduce the problem size
* $a$ is the number of recursive calls
* $\Theta(n^c)$ is the cost to prepare the recursive calls and combine their results

A simplified version of the master theorem states that:

$$T(n) \in \begin{cases}
    \color{red} \Theta(n^c) \color{black} & \text{if } c > \log_{b}(a) \hspace{0.6cm} \text{(root-heavy)} \\\\
    \color{green} \Theta(n^c \log(n)) \color{black} & \text{if } c = \log_{b}(a) \hspace{0.6cm} \text{(balanced)} \\\\
    \color{blue} \Theta(n^{\log_{b}(a)}) \color{black} & \text{if } c < \log_{b}(a) \hspace{0.6cm} \text{(leaf-heavy)}
    \end{cases} $$

Observe that there is $1$ root, $a$ children, $a^2$ grandchildren, and $a^k$ leaves.

We can argue that this works in cases where our subproblems may not have exact sizes (ie. in the case of floors/ceilings), allowing us to perform "lazy" evaluations. The proof for this fact is out of scope for this course.

#### Proof

A cleaner version of this theorem has $b^k$ in replacement of $n$. We can prove that $n = b^k$ using recursion unrolling, logarithm rules, and case analysis. Observe that:

$$
\begin{aligned}
T(b^k) &= aT(b^{k-1}) + b^{kc} \\\\
       &= a(aT(b^{k-2}) + b^{(k-1)c}) + b^{kc} \\\\
       &= a^2T(b^{k-2}) + ab^{(k-1)c} + b^{kc} \\\\
       &= ... \\\\
       &= a^{k} + a^{k-1}b^{c} + a^{k-2}b^{2c} + ... + ab^{(k-1)c} + b^{kc} \\\\
       &= a^k \left(1 + \frac{b^c}{a} + ... + \left(\frac{b^c}{a}\right)^k\right) \\\\
       &= (b^c)^k \left(1 + \frac{a}{b^c} + ... + \left(\frac{a}{b^c}\right)^k \right)
\end{aligned}
$$

For $n = b^k$, we have $k = \log_{b}(n)$, so $a^k = a^{\log_b(n)} = n^{\log_{b}(a)}$. Then, observe that:

* If $a = b^c$, then from the second last summation, we can successfully onclude that $T(n) = a^k (k + 1) = \color{green} n^c(\log_{b}(n) + 1)$
* If $a > b^c$, then we have a geometric series evident in the second last summation, which gives $a^k \leq T(n) \leq K_1 a^k = n^{\log_{b}(a)} \leq T(n) \leq K_1 \color{red} n^{\log_{b}(a)}$
* If $a < b^c$, then we have a geometric series evident in the last summation, which gives $(b^c)^k \leq T(n) \leq K_2 (b^c)^k = n^c \leq T(n) \leq K_2 \color{blue} n^c$

Observe that these only hold in the case where $n = b^k$.

#### Alternatives to Master Theorem

If we cannot apply the Master Theorem to a chosen recurrence, then we need to either use a recursion tree or unroll the recurrence. To use the recursion tree argument:

1. Draw the tree based on the recurrence given - the tree's leaves should all be $T(1)$
2. For each level of the tree, identify the size of the subproblems, number of nodes, cost/node, and total cost. It is helpful to identify common patterns in the numbers like shown with the Merge Sort example
3. Sum the costs of all levels of the recursion tree and determine the order

## Divide and Conquer

To solve a problem in size $n$ using the **Divide and Conquer** strategy, we:

1. **Divide**: Break the input into smaller problems, ideally into size $\frac{n}{b}$
2. **Conquer**: Solve these subproblems recursively
3. **Recombine**: Deduce the solution of the main problem from the subproblems

For the remainder of this section, we will see how we can solve some problems using the Divide and Conquer approach.

### Maximum Subarray (via. Divide and Conquer)

> **Problem Statement:** \
> Given an array $A[0..n-1]$, find a contiguous subarray $A[i..j]$ that maximizes the sum $A[i] + A[i+1] + ... + A[j-1] + A[j]$ and return this sum. \
> This corresponds to the "Maximum Subarray" problem on [LeetCode](https://leetcode.com/problems/maximum-subarray/).

A brute force solution to this problem is $\Theta(n^2)$ and consists of checking every range for the maximum sum. Instead, let's solve this problem using Divide and Conquer.

Here is our setup:

1. **Divide** the problem of size $n$ into two subproblems of size $\frac{n}{2}$ (the lower half and upper half of the array)
2. **Conquer** these subproblems recursively, determining the base case
3. **Recombine** the subproblems to determine our solution

We need to then carefully analyze step 3 - how do we actually use the halfs of the array to determine the solution? Observe that there are three cases here:
1. The maximum subarray lies in the lower half
2. The maximum subarray lies in the upper half
3. The maximum subarray starts in the lower half and ends in the upper half

In the third case, consider maximizing the lower and upper halves independently (get the maximum sum of these halves). Here is the solution in pseudocode.
```cpp
int maxSubArray(vector<int>& nums) {                  // Wrapper Function
    return maxSubArray(nums, 0, nums.size() - 1);
}

int maxSubArray(vector<int>& nums, int l, int u) {    // The Main Algorithm
    if (l == u) return nums[l];                       // Base Case

    int m = l + (u - l) / 2;                          // Calculate Middle

    int lmax = nums[m];                               // Get maximum sum of lower half
    for (int i = m, sum = 0; i >= l; --i) {           // O(n)
        sum += nums[i];
        if (sum > lmax) lmax = sum;
    }

    int rmax = nums[m + 1];                           // Get maximum sum of upper half
    for (int i = m + 1, sum = 0; i <= u; ++i) {       // O(n)
        sum += nums[i];
        if (sum > rmax) rmax = sum;
    }

    return max({ maxSubArray(nums, l, m),             // Divide, Conquer, Recombine
                 maxSubArray(nums, m + 1, u),         // T(n) = 2T(n/2)
                 lmax + rmax });                      //       (+ O(n) from before)
}
```
Now with this approach, we improve our time complexity to $\Theta(n \log_{2} n)$, as proven by applying the Master Theorem to the recurrence relation $T(n) = 2T(\frac{n}{2}) + \Theta(n)$. Observe that this is the same recurrence relation as Merge Sort.

For this problem specifically, there is actually a better solution to this that involves dynamic programming, and we will eventually get to it

### Karatsuba's Algorithm

> **Problem Statement:** \
> Given two polynomials $F = f_0 + ... + f_{n-1}x^{n-1}$ and $G = g_0 + ... + g_{n-1}x^{n-1}$, complete their product $FG$, given by: \
> $$ H = FG = f_0g_0 + (f_0g_1 + f_1g_0)x + ... + f_{n-1}g_{n-1}x^{2n-2} $$

As a prelimary, we will assume that $\forall i, |f_{i}| \leq K$ and $|g_{i}| \leq K$. Then, $|h_{i}| \leq nK^2$. In the word RAM model where the input size is $2^b$, the output requires at most $3b$ words in this model, which is $O(1)$. However, for our assignment where $K = 10^5$ and $n \leq 10^5$, every coefficient in the output will fit in 128 bits.

An easy brute force algorithm in $\Theta(n^2)$ would look like:
```
for i = 0 to n-1 do:
    for j = 0 to n-1 do:
        h{i + j} = h{i + j} + f{i}g{j}
```

However, Karatsuba provides a Divide and Conquer algorithm that allows the multiplication of polynomials in less than $\Theta(n^2)$.

#### The Algorithm

Suppose we cut a polynomial in half like this:
$$F(x) = 1 + 2x + 3x^2 + 4x^3 = 1 + 2x + x^2(3 + 4x)$$

More generally, we write $F = F_0 + F_1x^{\lfloor \frac{n}{2} \rfloor}$ and $G = G_0 + G_1x^{\lfloor \frac{n}{2} \rfloor}$. Then, $H$ is:

$$ H = F_0G_0 + (F_0G_1 + F_1G_0)x^{\lfloor \frac{n}{2} \rfloor} + F_1G_1x^{2 \cdot \lfloor \frac{n}{2} \rfloor}$$

Observe that we have 4 recursive calls in size $\lfloor \frac{n}{2} \rfloor$, and we have $\Theta(n)$ additions to compute $F_0G_1 + F_1G_0$ as well as the overlaps. We can use the following identity to adjust this to only 3 recursive calls coloured in red, green, and blue respectively.

$$
\begin{aligned}
& \hspace{0.2cm} (F_0 + F_1x^{\lfloor \frac{n}{2} \rfloor}) (G_0 + G_1x^{\lfloor \frac{n}{2} \rfloor}) \\\\
=& \hspace{0.2cm} \color{red} F_0 G_0 \color{black} + (\color{green} (F_0 + F_1)(G_0 + G_1) \color{black} - \color{red} F_0 G_0 \color{black} - \color{blue} F_1 G_1) \color{black} x^{\lfloor \frac{n}{2} \rfloor} + \color{blue} F_1 G_1 \color{black} x^{2 \cdot \lfloor \frac{n}{2} \rfloor}
\end{aligned}
$$

Our recurrence relation is then $T(n) = 3T\left(\frac{n}{2}\right) + \Theta(n)$. Applying the master theorem gives $T(n) \in \Theta(n^{\log_{2}(3)})$.

#### Example

Suppose we have the following polynomials of degree two, implying $n = 3$:
$$ F(x) = 1 + 2x + 3x^2 $$
$$ G(x) = 3 + 4x + 5x^2 $$

Since $\lfloor \frac{3}{2} \rfloor = 1$, we will factor $x$ from each term that contains $x$:

$$ F(x) = 1 + (2 + 3x)x $$
$$ G(x) = 3 + (4 + 5x)x $$

Note that $F_0 = 1$, $F_1 = 2 + 3x$, $G_0 = 3$, and $G_1 = 4 + 5x$.
Therefore, our subproblems are as follows:

* $\color{red} F_0G_0 \color{black} = 3$ 
* $\color{blue} F_1G_1 \color{black} = 8 + 22x + 15x^2$
* $\color{green} (F_0 + F_1)(G_0 + G_1) \color{black} = 21 + 36x + 15x^2$

Which applying the identity gives:

$$
\begin{aligned}
H(x) &= 3 + ((21 + 36x + 15x^2) - 3 - (8 + 22x + 15x^2))x + (8 + 22x + 15x^2)x^2 \\\\
&= 3 + 10x + 22x^2 + 22x^3 + 15x^4
\end{aligned}
$$


### Strassen's Algorithm

> **Problem Statement:** \
> Given two matrices $A = \[a_{i, j}\]$ and $B = \[b_{j, k}\]$, compute $C = AB$

Let $A$ and $B$ be 2-dimensional matrices, and observe that:
$$
C = 
\begin{pmatrix}
    A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2, 2} \\\\
    A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{2,2} + A_{2,2}B_{2, 2}
\end{pmatrix}
$$

With the brute force algorithm, we would have 8 recursive calls in size $\frac{n}{2}$ with $\Theta(n^2)$ additions. Strassen's algorithm allows us to calculate $C$ using only 7 recursive calls, giving a new time complexity of $T(n) \in \Theta(n^{\log_{2}(7)})$ in the case of two dimensional matrices. The exact details of the algorithm do not need to be memorized, and thus won't be presented. However, the algorithm itself is discussed because it shows another way you can utilize mathematical properties (like in the Karatsuba algorithm) to shorten the number of recursive calls.

It is good to keep note however that for matrix multiplication, any algorithm that does $k$ multiplications for matrices of size $\ell$ will always be $T(n) \in \Theta(n^{\log_{\ell}(k)})$.


### Counting Inversions

> **Problem Statement:** \
> Given an unsorted array $A$, find the number of inversions in it. Two indices $(i, j)$ is an inversion if $i < j$ and $A[i] > A[j]$.

As a guiding example, note that with $A = \[1, 5, 2, 6, 3, 8, 7, 4\]$:
$$|(2, 3), (2, 5), (2, 8), (4, 5), (4, 8), (6, 7), (6, 8), (7, 8)| = 8$$

We have an easy algorithm in $\Theta(n^2)$ to do this, but we can do better than this provided that we only have to _count_ the inversions and not _list_ them. 

Suppose for $n = 2^k$, we have:
* $c_{\ell}$ = number of inversions in the left half of the array
* $c_{r}$ = number of inversions in the right half of the array
* $c_t$ = number of **transverse** inversions with $i \leq \frac{n}{2}$ and $j > \frac{n}{2}$.

For the first few recursive calls, we have:
$$
\begin{aligned}
c_{\ell} = 1 &\rightarrow (2, 3) \\\\
c_{r} = 3 &\rightarrow (6, 7), (6, 8), (7, 8) \\\\
c_{t} = 4 &\rightarrow (2, 5), (2, 8), (4, 5), (4, 8)
\end{aligned}
$$

It is easy to recursively calculate $c_{\ell}$ and $c_{r}$, but extra care is required for $c_{t}$. We want to find this in $\Theta(n)$ to ensure an algorithm better than $\Theta(n^2)$.

To calculate $c_{t}$, note that $c_{t}$ is simply the number of $i$s greater than all elements in the right half of the array. Alternatively, $c_{t}$ is simply the number all $j$ less than all elements in the left half of the array.

$$A = \[\color{red}1, 5, 2, 6 \color{black}, \color{blue} 3, 8, 7, 4 \color{black}\]$$

Since $c_t$ is not dependent on the ordering, we can sort the left half of the array and then for each $i$ in the left half, we apply binary search in the right half of the array.

We have a recurrence relation $T(n) = 2T\left(\frac{n}{2}\right) + n\log(n)$, and unfortunately, we cannot apply the Master Theorem to solve this. If we unwind the recursion:

$$
\begin{aligned}
    T(n) &= 2T\left(\frac{n}{2}\right) + n\log(n) \\\\
         &= 2\left(2T\frac{n}{4} + \frac{n}{2}\log\left(\frac{n}{2}\right)\right) + n\log n \\\\
         &= 4T\left(\frac{n}{4}\right) + n\log\frac{n}{2} + n\log n \\\\
         &\leq 4T\left(\frac{n}{4}\right) + 2n\log n \\\\
         &\leq 8T\left(\frac{n}{8}\right) + 3n\log n \\\\
         &\leq nT(1) + n (\log n)^2
\end{aligned}
$$
We can see that $T(n) \in O(n\left(\log n\right)^2)$. This is still not ideal, however. An even better idea is to perform Merge Sort **while** performing the recursive calls for $c_{\ell}$ and $c_{r}$.

```a
Merge(A)
    copy A to new array S; c = 0;
    i = 1; j = n/2 + 1;
    for (k = 1; k <= n; k++) do
        if (j > n/2) then A[k] = S[j++];
        else if (j > n) then
            A[k] = S[i++];
            c = c + n/2;
        else if (S[i] < S[j]) then
            A[k] = S[i++];
            c = c + j - (n/2 + 1);
        else A[k] = S[j++];
```

Then, this is just a modification of the Merge Sort algorithm and our final solution will be in $\Theta(n \log n)$. Keep in mind that $\Omega(n \log(n))$ is a lower bound as we have to perform comparisons.

### Finding Closest Pair of Points
> **Problem Statement:** \
> Given $n$ points $(x_i, y_i)$ in the plane, find a pair $(i, j)$ that minimizes the distance.

To be done independently before midterms begin.









### Median of Medians
> **Problem Statement:** \
> Given $A$, find the entry that would be at index $\lfloor \frac{n}{2} \rfloor$ if $A$ was sorted.

This is a variant of the selection problem where we need to find the entry that would be at $k$. We could sort to determine this or use a randomized algorithm which depends on luck.

However, there is a linear algorithm to the selection problem called QuickSelect:

```cpp
int partition(int arr[], int l, int r) {
    int x = arr[r], i = l;
    for (int j = l; j <= r - 1; j++) {
        if (arr[j] <= x) {
            swap(arr[i], arr[j]);
            i++;
        }
    }
    swap(arr[i], arr[r]);
    return i;
}

int quickSelect(int arr[], int l, int r, int k) {
    if (k > 0 && k <= r - l + 1) {
        int index = partition(arr, l, r);
  
        if (index - l == k - 1)
            return arr[index];
  
        if (index - l > k - 1) 
            return quickSelect(arr, l, index - 1, k);
  
        return quickSelect(arr, index + 1, r, 
                            k - index + l - 1);
    }
    return INT_MAX;
}
```

The question then becomes trying to find a pivot such that both $i$ and $n - i - 1$ are not too large. To do this:
* Divide $A$ into $\frac{n}{5}$ groups $G_1, ... , G_{\frac{n}{5}}$ of size 5
* Find the medians $m_1, ... , m_{\frac{n}{5}}$ of each group
* pivot $p$ is the median of $\[m_1, ... , m_{\frac{n}{5}}\]$

With this choice of $p$, our indices $i$ and $n - i - 1$ will be at most $\frac{7n}{10}$. This follows from the fact there are 3 elements in $G_i$ greater than or equal to $m_{i}$.

Our runtime then satisfies:

$$T(n) \leq T\left(\frac{n}{5}\right) + T\left(\frac{7n}{10}\right) + O(n)$$

Not only will the Master Theorem **not** work, but the other strategies (loop unrolling, recursion trees) will be way too messy. Instead, let's propose that $T(n) \in O(n)$. A guess and check argument implies that:

$$T(n) \leq kn \implies T(n) \leq \frac{kn}{5} + \frac{7kn}{10} + \alpha n = \left(\alpha + \frac{9k}{10}\right)n$$

Then, we can take $k = 10\alpha$ to let $\alpha + \frac{9k}{10} = k$. We then see that $T(n) \in O(n)$, albeit with a very high constant.

Taking the median of five gives us optimal results - if we chose median of threes, then using a diagram, we can see that $T(n) \leq T\left(\frac{n}{3}\right) + T\left(\frac{2n}{3}\right) + O(n) \in O(n \log n)$.






Â 


# Graph Algorithms

We will focus on both undirected and directed graphs. We wish to study breadth-first search and depth-first search and see its applications (shortest path, bipartite matching, cycle detection, cut vertices, etc.)

## Undirected Graphs

From MATH239, we know that a graph $G$ is a pair $(V, E)$ where:
* $V$ is a finite set of size $n$ whose elements are called vertices
* $E$ is a finite set of size $m$ whose elements are unordered pairs of distinct vertices. These are called the edges of $G$

To represent these using a data structure, we can use an adjacency list. This is an array $A[1..n]$ such that $A[v]$ is the linked list (or any other collection) of all edges connected to $v$. It will have size $\Theta(n + m)$ and testing if an edge exists will not be $O(1)$.

The definition of $G$ provided here generates an undirected graph where the edges do not have any "direction" and simply represent a connection.

Some key definitions:

1. **Path:** A sequence $v_1, ... , v_k$ of vertices with $(v_i, v_{i+1}) \in E$ for all $i$
2. **Connected Graph:** For all $v, w \in V$, there exists a path $v \to w$
3. **Cycle:** A path $v_1, ... , v_k, v_1$ with $k \geq 3$ and $v_i$'s pairwise distinct
4. **Tree:** A connected graph without any cycle
5. **Rooted Tree:** A tree with a special vertex called root
6. **Subgraph:** A graph $G' = (V', E')$ where $V' \subset V$ and $E' \subset E$ with all $e \in E'$ joining vertices $u, v \in V'$
7. **Connected Component:** A connected subgraph of $G$ not contained in a larger connected subgraph of $G$

## Breadth-First Search

### Main Idea

To perform a breadth-first search, we visit every node at the present depth prior to moving onto the nodes at the next depth. We can define depth as the distance away from $s$.

<center>
<img src="/cs341-bfs.png" width="280" height="200" alt="BFS Diagram">
</center>


### Pseudocode
```a
G is a graph with n vertices implemented with an adjacency list
s is a vertex from G

BFS(G, s):
    let Q be an empty queue
    let visited be an array of size n with all entries set to false
    enqueue(s, Q)
    visited[s] = true
    while Q is not empty:
        v = dequeue(Q)
        for all w neighbours of v:
            if visited[w] is false:
                enqueue(w, Q)
                visited[w] = true
```

### Correctness + Time Complexity

To prove that this algorithm is correct, we first need to show that for all $v$, if $visited[v]$ is true at the end, then there exists a path $s \to v$ in $G$. A proof by induction will be used.

> For $i = 0$, this holds true. So then suppose true for $v_0, ..., v_{i - 1}$. When $v_i$ is set to true during the algorithm, we are looking at the neighbours of a certain $v_{j}$ that we already visited. By assumption, there is a path $s \to v_j$. Because $(v_j, v_i) \in E$, then there must exist a path $s \to v_i$.

We also need to prove that for all vertices $v$, if there is a path $s \to v$ in $G$, then $visited[v]$ is true at the end. A proof by induction will be used.

> Let $v_0 = s, ..., v_k = v$. We know that $visited[v_0]$ is true. So, then suppose $visited[v_i]$ is true. Then, we would examine all neighbours $v_j$ of $v_i$, of which we would set $visited[v_j] = true$ for each $v$. Note that this implies $visited[v_{i+1}]$ will be true, which then implies that $visited[v]$ will be true.

Therefore, in general, we have that for all vertices $v$, there is a path $s \to v$ if and only if $visited[v]$ is true at the end.

As for time complexity, note that each vertex is enqueued, dequeued, and read at most once. Therefore, will do a maximum of $n$ passes for the while loop. Then, using the convention that $d_v$ is the number of neighbours of $v$, we can apply the handshaking lemma to find the for loop is:

$$O\left(\sum_{v} d_v \right) = O(m)$$

This traversal alone will therefore have a cost $O(n + m)$.


### BFS Tree

### Shortest Path via. BFS Trees

We can visualize the paths from $s$ to $v$ as traversing through a tree where each level corresponds to the distance away from $s$. Note that:

1. The levels in the queue are non-decreasing
2. For all vertices $u, v$, if there is an edge $(u, v)$, then $level[v] \leq level[v] + 1$

We can then conclude that for all $v$ in $G$, there is a path $s \to v$ in $G$ if and only if there is a path $s \to v$ in $T$. If so, then the path in $T$ is a shortest path and $level[v] = dist(s, v)$

### Bipartite Testing via. BFS Trees

Recall that a graph $G = (V, E)$ is bipartite if there a partition $V = V_1 \cup V_2$ such that all edges $e \in E$ have one end in $V_1$ and one end in $V_2$.

Assuming that $G$ is connected, we can run BFS from any $s$ and set $V_1$ to be vertices with odd level and $V_2$ with vertices with even level. If there exists any edges between two vertices in $V_1$ or $V_2$, then $G$ is not bipartite. Otherwise, it is bipartite.

<center>
<img src="/tree2.png" width="220" height="190" alt="Tree Bipartition">
</center>


Note that this is testable in $O(n + m)$.


## Depth First Search 

### Main Idea

To perform a depth-first search, we select a node and travel as deep as possible, then backtrack (go to a previously visited node that still has children to visit) when we cannot go any further.

<center>
<img src="/cs341-dfs.png" width="280" height="200" alt="DFS Diagram">
</center>

### Pseudocode (Recursive)

```a
G is a graph with n vertices implemented with an adjacency list

DFS(G):
    for all v in G:
        if visited[v] is false
            explore(v)

explore(v):
    visited[v] = true
    for all w neighbour of v:
        if visited[w] = false:
            explore(w)
```

### Correctness + Time Complexity

To prove correctness of this algorithm, we need to prove the white path lemma - when we start exploring a vertex $v$, any $w$ that can be connected to $v$ by an unvisited path will be visited once $explore(v)$ is finished.

> Let $v_0 = v_1, ... , v_k = w$ be a path $v \to w$ with $v_1, ... , v_k$ not visited yet. For $i = 0$, this holds true. So then suppose true for $i < k$. When we visit $v_i$, $explore(v)$ is not finished and $v_{i + 1}$ is one of its neighbours. Either $visited[v_{i+1}]$ is true or we will visit it now, which means that it will be done before $explore(v)$ is finished.

Our BFS arguments allow for the rest of the required correctness proofs concerning paths. The runtime of DFS is the same as for BFS, which is $O(n + m)$.


### DFS Trees

### Cut Vertices via. DFS Trees

We say that $G$ is biconnected if $G$ is connected and $G$ stays connected if we remove any vertex (and all edges that contain it). If $G$ is not biconnected, then there exists a vertex $v$ such that removing it makes $G$ disconnected. This $v$ is known as a cut vertex (or articulation point).

Let's find an algorithm to find cut vertices. Our simplest case is that the root $s$ could be a cut vertex. This occurs if and only if it has more than one child. If $s$ has one child, then removing $s$ will leave $T$ connected. However, if $s$ has more than one child, then $s$ will have subtrees $S_1, S_k$, $k > 1$ and because there's no edge connecting $S_i$ to $S_j$, $i \neq j$, removing $s$ will create $k$ connected components. (to do: Why?)

Now let's consider the case where a non-root vertex is a cut vertex. Define:

* $a(v) = \min \\\{ \text{level}[w], \\\{ v, w  \\\} \text{ edge} \\\}$
* $m(v) = \min \\\{ a(w), w \text{ descendant  of } v \\\}$

Any non-root vertex $v$ is a cut vertex if and only if it has a child $w$ with $m(w) \geq \text{level}[v]$. To see why, define $w$ as a child of $v$, $T_w$ be a subtree at $w$, and $T_v$ be a subtree at $v$. Then:

* If $m(w) < \text{level}[v]$, there is an edge from $T_w$ to a vertex above $v$, so $T_w$ would still be connected
* If $m(w) \geq \text{level}[v]$, all edges originate from $T_w$ end in $T_v$, and because every edge originates from $x \in T_w$ and ends at a level at least $\text{level}[v]$, then removing $v$ will mean that $T_w$ is disconnected from the root.

For some vertex $v$, we can compute $m(v)$ in $O(m)$ and test the condition in $O(d_v)$, so the total time complexity is $O(m)$. This follows from an observation that if $v$ has children $w_1, ... , w_k$, then $m(v) = \min \\\{ a(v), m(w_1), ... , m(w_k) \\\}$


## Directed Graphs

Our algorithms for BFS and DFS work without any change. For this course, we'll be concerned primarily with DFS.

Consider the following `explore(v)` implementation:

```a
explore(v):
    visited[v] = true
    start[v] = t, t++
    for all w neighbour of v do:
        if visited[w] = false:
            explore(w)
    finish[v] = t, t++
```

Observe that:

* If $w$ is not finished, $(v, w)$ is a back edge
* If start[v] < start[w] < finish[w], $(v, w)$ is a forward edge
* Else we have start[w] < finish[w] < start[v], $(v, w)$ is a cross edge