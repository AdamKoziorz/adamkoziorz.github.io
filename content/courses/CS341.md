---
title: "CS341 - Algorithms"
url: "courses/CS341/"
summary: The study of efficient algorithms and effective algorithm design techniques
ShowToc: true
math: true
---

> **Instructor:** Eric Schost \
> **Lectures:** T/Th at 8:30am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 5 $\times$ 8% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 40% \
> \
> **Course Description:** \
> The study of efficient algorithms and effective algorithm design techniques. Program design with emphasis on pragmatic and mathematical aspects of program efficiency. Topics include divide and conquer algorithms, recurrences, greedy algorithms, dynamic programming, graph search and backtrack, problems without algorithms, NP-completeness and its implications.


## Introduction

Building upon our CS240 knowledge of data structures and Big-O notation, as well as prior math, we will learn many algorithms (divide-and-conquer, BFS, DFS, greedy, dynamic programming, flows and cuts) and their pseudocode, correctness, and runtime. We will also briefly discuss NP-completeness and reductions.

### Prerequisite Knowledge

#### Runtime Definitions
Recall that the input (problem instance) of an algorithm is parameterized by $n$ called the size. We then had the following brief definitions:

$T(I)$ - $\text{Runtime on input } I$

$T(n)$ - $\max_{I \text{ of size } n} T(I)$

$T_{avg}(n) - \frac{\sum_{I \text{ of size } n} T(I)}{\text{number of inputs of size } I}$

#### Asymptotic Notation

Consider two functions $f(n)$, $g(n)$ with values in $\mathbb{R}_{>0}$. Then, we have:
* $f(n) \in O(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \leq Cg(n)$
* $f(n) \in \Omega(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \geq Cg(n)$
* $f(n) \in \Theta(g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$
* $f(n) \in o(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) < Cg(n)$
* $f(n) \in \omega(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) > Cg(n)$

In the case of graphs and matrices, our algorithms may contain two parameters $n$ and $m$. We need to redefine our asymptotic notation to accommodate two parameters. For big-$O$ notation, we have the following definition:

$f(n, m) \in O(g(n, m))$ if $\exists C, n_0, m_0$ such that $f(n, m) \leq Cg(n, m)$ for $n \geq n_0$ or $m \geq m_0$

#### Asymptotic Analysis 
Here are some useful examples of asymptotic analysis that will be standard for the course going forwards:
* $n^k + c_{k-1}n^{k-1} + ... + c_0 \in \Theta(n^k)$
* $n^{O(1)}$ means (at most) polynomial in $n$
* $n \log(n) \in O(n^2), \Omega(n)$
* $2^{n-1} \in \Omega(2^n)$
* $(n-1)! \in o(n!)$ and NOT $\Theta(n!)$

Here are some practical notes for dealing with asymptotic analysis:
1. Big-$O$ is only an upper bound, and we should aim to give $\Theta$'s if possible.
2. Big-Anything hides constants and only states that one algorithm will _eventually_ beat another algorithm (ie. $\Theta(n^2)$ could be faster than $\Theta(n)$ for small enough input)
3. We will use a simplified model, focusing purely on "operations"

#### Computational Model (word RAM)

Memory locations contain integer words of $b$ bits each, and $b \geq \log(n)$ for input size $n$.

Random Access Memory can access any memory location at unit cost, and all basic operations on words (loop counters, array indices, etc.) have unit costs. Unless otherwise implemented, we will always assume that these basic operations are constant time.

It is possible to perform run-time analysis in terms of integer words using the word RAM model, though we don't have to worry about this.


## Divide and Conquer Algorithms

### Recurrences

Let's investigate Merge Sort, which uses divide and conquer to sort an array. With this sorting algorithm, we split the array into halves, sort those halves, then merge. Observe that merging requires at most $n - 1$ comparisons and can be done in-place using the two pointers strategy. Merge sort follows this recurrence relation:

$$ t(1) = 0, \hspace{0.4cm} t(n) = 2t\left(\frac{n}{2}\right) + n - 1$$

The $n - 1$ is not very convinent to analyze, so let's instead define:

$$ T(1) = 0, \hspace{0.4cm} T(n) = 2T\left(\frac{n}{2}\right) + n$$

and observe that $t(n) \leq T(n)$ by induction.

Let's unroll the recurrence relation:

$$
\begin{aligned}
    T(n) &= 2T\left(\frac{n}{2}\right) + n \\\\
         &= 2\left(2T\left(\frac{n}{4}\right) + \frac{n}{2}\right) + n \\\\
         &= 4T\left(\frac{n}{4}\right) + 2n \\\\
         &= 8T\left(\frac{n}{8}\right) + 3n \\\\
         &= ... \\\\
         &= nT\left(\frac{n}{n}\right) + \log_{2}(n) n \\\\
         &= nT(1) + n\log_{2}(n) \\\\
         &= n\log_{2}(n) \\\\
\end{aligned}
$$

Thus, we can see that merge sort is $\Theta(n \log_{2}(n))$. How do we handle more complicated recurrence relations?

### Master Theorem

Consider two integers $a, b \geq 1$, a real number $c \geq 0$, and a function $T(n)$ such that:

$$ T(1) = c, \hspace{0.4cm} T(n) = aT\left(\frac{n}{b}\right) + \Theta(n^c)$$

where $n$ is a power of $b$. Observe that:
* $b$ is the factor by which we reduce the problem size
* $a$ is the number of recursive calls
* $\Theta(n^c)$ is the cost to prepare the recursive calls and combine their results

Then, the master theorem (cleanly) states that:

$$T(b^k) = \begin{cases}
    \Theta((b^k)^c) & \text{if } a < b^c \iff c > \log_{b}(a) \\\\
    \Theta((b^k)^c \log(b^k)) & \text{if } a = b^c \iff c = \log_{b}(a) \\\\
    \Theta((b^k)^{\log_{b}(a)}) & \text{if } a > b^c \iff c < \log_{b}(a)
    \end{cases} $$

A dirty version of this theorem replaces $b^k$ with $n$. In fact, we can prove that $n = b^k$ using recursion unrolling, logarithm rules, and case analysis. The proof has been omitted.



