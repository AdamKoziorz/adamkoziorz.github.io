---
title: "CS341 - Algorithms"
url: "courses/CS341/"
summary: The study of efficient algorithms and effective algorithm design techniques
ShowToc: true
math: true
---

> **Instructor:** Eric Schost \
> **Lectures:** T/Th at 8:30am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 5 $\times$ 8% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 40% \
> \
> **Course Description:** \
> The study of efficient algorithms and effective algorithm design techniques. Program design with emphasis on pragmatic and mathematical aspects of program efficiency. Topics include divide and conquer algorithms, recurrences, greedy algorithms, dynamic programming, graph search and backtrack, problems without algorithms, NP-completeness and its implications.

<br/>

# Introduction

Building upon our CS240 knowledge of data structures and Big-O notation, as well as prior math, we will learn many algorithms (divide-and-conquer, BFS, DFS, greedy, dynamic programming, flows and cuts) and their pseudocode, correctness, and runtime. We will also briefly discuss NP-completeness and reductions.

## Prerequisite Knowledge

### Runtime Definitions
Recall that the input (problem instance) of an algorithm is parameterized by $n$ called the size. We then had the following brief definitions:

$T(I) = \text{Runtime on input } I$

$T(n) = \max_{I \text{ of size } n} T(I)$

$T_{avg}(n) = \frac{\sum_{I \text{ of size } n} T(I)}{\text{number of inputs of size } I}$

### Asymptotic Notation

Consider two functions $f(n)$, $g(n)$ with values in $\mathbb{R}_{>0}$. Then, we have:
* $f(n) \in O(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \leq Cg(n)$
* $f(n) \in \Omega(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \geq Cg(n)$
* $f(n) \in \Theta(g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$
* $f(n) \in o(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) < Cg(n)$
* $f(n) \in \omega(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) > Cg(n)$

In the case of graphs and matrices, our algorithms may contain two parameters $n$ and $m$. We need to redefine our asymptotic notation to accommodate two parameters. For big-$O$ notation, we have the following definition:

$f(n, m) \in O(g(n, m))$ if $\exists C, n_0, m_0$ such that for $n \geq n_0$ or $m \geq m_0$, we have $f(n, m) \leq Cg(n, m)$

### Examples of Asymptotic Analysis
Here are some useful examples of asymptotic analysis that will be standard for the course going forwards:
* $n^k + c_{k-1}n^{k-1} + ... + c_0 \in \Theta(n^k)$
* $n^{O(1)}$ means (at most) polynomial in $n$
* $n \log(n) \in O(n^2), \Omega(n)$
* $2^{n-1} \in \Omega(2^n)$
* $(n-1)! \in o(n!)$ and NOT $\Theta(n!)$

Here are some practical notes for dealing with asymptotic analysis:
1. Big-$O$ is only an upper bound, and we should aim to give $\Theta$'s if possible.
2. Big-Anything hides constants and only states that one algorithm will _eventually_ beat another algorithm (ie. $\Theta(n^2)$ could be faster than $\Theta(n)$ for small enough input)
3. We will use a simplified model, focusing purely on "operations"

### Computational Model (word RAM)

Memory locations contain integer words of $b$ bits each, and $b \geq \log(n)$ for input size $n$.

Random Access Memory can access any memory location at unit cost, and all basic operations on words (loop counters, array indices, etc.) have unit costs. Unless otherwise implemented, we will always assume that these basic operations are constant time.

It is possible to perform run-time analysis in terms of integer words using the word RAM model, though we don't have to worry about this.

<br/>

# Divide and Conquer Algorithms

## Recurrences

Let's investigate Merge Sort, which uses divide and conquer to sort an array. With this sorting algorithm, we split the array into halves, sort those halves, then merge. Observe that merging requires at most $n - 1$ comparisons and can be done in-place using the two pointers strategy. Merge sort follows this recurrence relation:

$$ t(1) = 0, \hspace{0.4cm} t(n) = 2t\left(\frac{n}{2}\right) + n - 1$$

The $n - 1$ is not very convenient to analyze, so let's instead define:

$$ T(1) = 0, \hspace{0.4cm} T(n) = 2T\left(\frac{n}{2}\right) + n$$

and observe that $t(n) \leq T(n)$ by induction.

Let's unroll the recurrence relation:

$$
\begin{aligned}
    T(n) &= \color{red}{2} \color{black}T\left(\frac{n}{\color{red}{2} \color{black}}\right) + \color{red}{1} \color{black}n \\\\
         &= 2\left(2T\left(\frac{n}{4}\right) + \frac{n}{2}\right) + n \\\\
         &= \color{red}{4} \color{black}T\left(\frac{n}{\color{red}{4} \color{black}}\right) + \color{red}{2} \color{black}n \\\\
         &= \color{red}{8} \color{black}T\left(\frac{n}{\color{red}{8} \color{black}}\right) + \color{red}{3} \color{black}n \\\\
         &= ... \\\\
         &= \color{red}{n} \color{black}T\left(\frac{n}{\color{red}{n} \color{black}}\right) + \color{red}{\log_{2}(n)} \color{black} n \\\\
         &= nT(1) + n\log_{2}(n) \\\\
         &= n\log_{2}(n) \\\\
\end{aligned}
$$

Thus, we can see that merge sort is $O(n \log_{2}(n))$. If you look at the recursion tree for Merge Sort below, we get the same result:

![Recursion-Tree](/recursion-tree-example.png)

This recursion tree demonstrates how the problems are split up until we get $\frac{n}{2^k} = 1$ and how we add up the work on each level. The tree here has height $\log_2(n)$ and on each level, we perform a $O(n)$ operation, so the total work done is $O(n \log_2(n))$.

Note that with Merge Sort, this is actually a tight bound, so Merge Sort has $\Theta(n\log_{2}(n))$ time complexity. The question then becomes, how do we handle more complicated recurrence relations?

## Master Theorem

Consider two integers $a, b \geq 1$, a real number $c \geq 0$, and a function $T(n)$ such that:

$$ T(1) = C, \hspace{0.4cm} T(n) = aT\left(\frac{n}{b}\right) + \Theta(n^c)$$

where $n$ is a power of $b$. Observe that:
* $b$ is the factor by which we reduce the problem size
* $a$ is the number of recursive calls
* $\Theta(n^c)$ is the cost to prepare the recursive calls and combine their results

A simplified version of the master theorem states that:

$$T(n) \in \begin{cases}
    \color{red} \Theta(n^c) \color{black} & \text{if } c > \log_{b}(a) \hspace{0.6cm} \text{(root-heavy)} \\\\
    \color{green} \Theta(n^c \log(n)) \color{black} & \text{if } c = \log_{b}(a) \hspace{0.6cm} \text{(balanced)} \\\\
    \color{blue} \Theta(n^{\log_{b}(a)}) \color{black} & \text{if } c < \log_{b}(a) \hspace{0.6cm} \text{(leaf-heavy)}
    \end{cases} $$

Observe that there is $1$ root, $a$ children, $a^2$ grandchildren, and $a^k$ leaves.

We can argue that this works in cases where our subproblems may not have exact sizes (ie. in the case of floors/ceilings), allowing us to perform "lazy" evaluations. The proof for this fact is out of scope for this course.

### Proof

A cleaner version of this theorem has $b^k$ in replacement of $n$. We can prove that $n = b^k$ using recursion unrolling, logarithm rules, and case analysis. Observe that:

$$
\begin{aligned}
T(b^k) &= aT(b^{k-1}) + b^{kc} \\\\
       &= a(aT(b^{k-2}) + b^{(k-1)c}) + b^{kc} \\\\
       &= a^2T(b^{k-2}) + ab^{(k-1)c} + b^{kc} \\\\
       &= ... \\\\
       &= a^{k} + a^{k-1}b^{c} + a^{k-2}b^{2c} + ... + ab^{(k-1)c} + b^{kc} \\\\
       &= a^k \left(1 + \frac{b^c}{a} + ... + \left(\frac{b^c}{a}\right)^k\right) \\\\
       &= (b^c)^k \left(1 + \frac{a}{b^c} + ... + \left(\frac{a}{b^c}\right)^k \right)
\end{aligned}
$$

For $n = b^k$, we have $k = \log_{b}(n)$, so $a^k = a^{\log_b(n)} = n^{\log_{b}(a)}$. Then, observe that:

* If $a = b^c$, then from the second last summation, we can successfully onclude that $T(n) = a^k (k + 1) = \color{green} n^c(\log_{b}(n) + 1)$
* If $a > b^c$, then we have a geometric series evident in the second last summation, which gives $a^k \leq T(n) \leq K_1 a^k$, or rather, $n^{\log_{b}(a)} \leq T(n) \leq K_1 \color{red} n^{\log_{b}(a)}$
* If $a < b^c$, then we have a geometric series evident in the last summation, which gives $(b^c)^k \leq T(n) \leq K_2 (b^c)^k$, or rather, $n^c \leq T(n) \leq K_2 \color{blue} n^c$

Observe that these only hold in the case where $n = b^k$.


### Alternatives

If we cannot apply the Master Theorem to a chosen recurrence, then we need to either use a recursion tree or unroll the recurrence. To use the recursion tree argument:

1. Draw the tree based on the recurrence given - the tree's leaves should all be $T(1)$
2. For each level of the tree, identify the size of the subproblems, number of nodes, cost/node, and total cost. It is helpful to identify common patterns in the numbers like shown with the Merge Sort example
3. Sum the costs of all levels of the recursion tree and determine the order




## Divide and Conquer

To solve a problem in size $n$ using the **Divide and Conquer** strategy, we:

1. **Divide**: Break the input into smaller problems, ideally into size $\frac{n}{b}$
2. **Conquer**: Solve these subproblems recursively
3. **Recombine**: Deduce the solution of the main problem from the subproblems

For the remainder of this section, we will see how we can solve some problems using the Divide and Conquer approach.

### Maximum Subarray (via. Divide and Conquer)

> **Problem Statement:** \
> Given an array $A[0..n-1]$, find a contiguous subarray $A[i..j]$ that maximizes the sum $A[i] + A[i+1] + ... + A[j-1] + A[j]$ and return this sum. \
> This corresponds to the "Maximum Subarray" problem on [LeetCode](https://leetcode.com/problems/maximum-subarray/).

A brute force solution to this problem is $\Theta(n^2)$ and consists of checking every range for the maximum sum. Instead, let's solve this problem using Divide and Conquer.

Here is our setup:

1. **Divide** the problem of size $n$ into two subproblems of size $\frac{n}{2}$ (the lower half and upper half of the array)
2. **Conquer** these subproblems recursively, determining the base case
3. **Recombine** the subproblems to determine our solution

We need to then carefully analyze step 3 - how do we actually use the halfs of the array to determine the solution? Observe that there are three cases here:
1. The maximum subarray lies in the lower half
2. The maximum subarray lies in the upper half
3. The maximum subarray starts in the lower half and ends in the upper half

In the third case, consider maximizing the lower and upper halves independently (get the maximum sum of these halves). Here is the solution in pseudocode.
```cpp
int maxSubArray(vector<int>& nums) {                  // Wrapper Function
    return maxSubArray(nums, 0, nums.size() - 1);
}

int maxSubArray(vector<int>& nums, int l, int u) {    // The Main Algorithm
    if (l == u) return nums[l];                       // Base Case

    int m = l + (u - l) / 2;                          // Calculate Middle

    int lmax = nums[m];                               // Get maximum sum of lower half
    for (int i = m, sum = 0; i >= l; --i) {           // O(n)
        sum += nums[i];
        if (sum > lmax) lmax = sum;
    }

    int rmax = nums[m + 1];                           // Get maximum sum of upper half
    for (int i = m + 1, sum = 0; i <= u; ++i) {       // O(n)
        sum += nums[i];
        if (sum > rmax) rmax = sum;
    }

    return max({ maxSubArray(nums, l, m),             // Divide, Conquer, Recombine
                 maxSubArray(nums, m + 1, u),         // T(n) = 2T(n/2)
                 lmax + rmax });                      //       (+ O(n) from before)
}
```
Now with this approach, we improve our time complexity to $\Theta(n \log_{2} n)$, as proven by applying the Master Theorem to the recurrence relation $T(n) = 2T(\frac{n}{2}) + \Theta(n)$. Observe that this is the same recurrence relation as Merge Sort.

For this problem specifically, there is actually a better solution to this that involves dynamic programming, and we will eventually get to it.

### Karatsuba's Algorithm

> **Problem Statement:** \
> Given two polynomials $F = f_0 + ... + f_{n-1}x^{n-1}$ and $G = g_0 + ... + g_{n-1}x^{n-1}$, complete their product $FG$, given by: \
> $$ H = FG = f_0g_0 + (f_0g_1 + f_1g_0)x + ... + f_{n-1}g_{n-1}x^{2n-2} $$

As a prelimary, we will assume that $\forall i, |f_{i}| \leq K$ and $|g_{i}| \leq K$. Then, $|h_{i}| \leq nK^2$. In the word RAM model where the input size is $2^b$, the output requires at most $3b$ words in this model, which is $O(1)$.

An easy brute force algorithm in $\Theta(n^2)$ would look like:
```
for i = 0 to n-1 do:
    for j = 0 to n-1 do:
        h{i + j} = h{i + j} + f{i}g{j}
```

However, Karatsuba provides a Divide and Conquer algorithm that allows the multiplication of polynomials in less than $\Theta(n^2)$.

#### The Algorithm

Suppose we cut a polynomial in half like this:
$$F(x) = 1 + 2x + 3x^2 + 4x^3 = 1 + 2x + x^2(3 + 4x)$$

More generally, we write $F = F_0 + F_1x^{\lfloor \frac{n}{2} \rfloor}$ and $G = G_0 + G_1x^{\lfloor \frac{n}{2} \rfloor}$. Then, $H$ is:

$$ H = F_0G_0 + (F_0G_1 + F_1G_0)x^{\lfloor \frac{n}{2} \rfloor} + F_1G_1x^{2 \cdot \lfloor \frac{n}{2} \rfloor}$$

Observe that we have 4 recursive calls in size $\lfloor \frac{n}{2} \rfloor$, and we have $\Theta(n)$ additions to compute $F_0G_1 + F_1G_0$ as well as the overlaps. We can use the following identity to adjust this to only 3 recursive calls coloured in red, green, and blue respectively.

$$
\begin{aligned}
& \hspace{0.2cm} (F_0 + F_1x^{\lfloor \frac{n}{2} \rfloor}) (G_0 + G_1x^{\lfloor \frac{n}{2} \rfloor}) \\\\
=& \hspace{0.2cm} \color{red} F_0 G_0 \color{black} + (\color{green} (F_0 + F_1)(G_0 + G_1) \color{black} - \color{red} F_0 G_0 \color{black} - \color{blue} F_1 G_1) \color{black} x^{\lfloor \frac{n}{2} \rfloor} + \color{blue} F_1 G_1 \color{black} x^{2 \cdot \lfloor \frac{n}{2} \rfloor}
\end{aligned}
$$

Our recurrence relation is then $T(n) = 3T\left(\frac{n}{2}\right) + \Theta(n)$. Applying the master theorem gives $T(n) \in \Theta(n^{\log_{2}(3)})$.

#### Example

Suppose we have the following polynomials of degree two, implying $n = 3$:
$$ F(x) = 1 + 2x + 3x^2 $$
$$ G(x) = 3 + 4x + 5x^2 $$

Since $\lfloor \frac{3}{2} \rfloor = 1$, we will factor $x$ from each term that contains $x$:

$$ F(x) = 1 + (2 + 3x)x $$
$$ G(x) = 3 + (4 + 5x)x $$

Note that $F_0 = 1$, $F_1 = 2 + 3x$, $G_0 = 3$, and $G_1 = 4 + 5x$.
Therefore, our subproblems are as follows:

* $\color{red} F_0G_0 \color{black} = 3$ 
* $\color{blue} F_1G_1 \color{black} = 8 + 22x + 15x^2$
* $\color{green} (F_0 + F_1)(G_0 + G_1) \color{black} = 21 + 36x + 15x^2$

Which applying the identity gives:

$$
\begin{aligned}
H(x) &= 3 + ((21 + 36x + 15x^2) - 3 - (8 + 22x + 15x^2))x + (8 + 22x + 15x^2)x^2 \\\\
&= 3 + 10x + 22x^2 + 22x^3 + 15x^4
\end{aligned}
$$


### Strassen's Algorithm

> **Problem Statement:** \
> Given two matrices $A = \[a_{i, j}\]$ and $B = \[b_{j, k}\]$, compute $C = AB$

Let $A$ and $B$ be 2-dimensional matrices, and observe that:
$$
C = 
\begin{pmatrix}
    A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2, 2} \\\\
    A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{2,2} + A_{2,2}B_{2, 2}
\end{pmatrix}
$$

With the brute force algorithm, we would have 8 recursive calls in size $\frac{n}{2}$ with $\Theta(n^2)$ additions. Strassen's algorithm allows us to calculate $C$ using only 7 recursive calls, giving a new time complexity of $T(n) \in \Theta(n^{\log_{2}(7)})$ in the case of two dimensional matrices. The exact details of the algorithm do not need to be memorized, and thus won't be presented. However, the algorithm itself is discussed because it shows another way you can utilize mathematical properties (like in the Karatsuba algorithm) to shorten the number of recursive calls.

It is good to keep note however that for matrix multiplication, any algorithm that does $k$ multiplications for matrices of size $\ell$ will always be $T(n) \in \Theta(n^{\log_{\ell}(k)})$.


### Counting Inversions

> **Problem Statement:** \
> Given an unsorted array $A$ of size $n$, find the number of inversions in it. Two indices $(i, j)$ is an inversion if $i < j$ and $A[i] > A[j]$.

As a guiding example, note that with $A = \[1, 5, 2, 6, 3, 8, 7, 4\]$:
$$|(2, 3), (2, 5), (2, 8), (4, 5), (4, 8), (6, 7), (6, 8), (7, 8)| = 8$$

We have an easy algorithm in $\Theta(n^2)$ to do this, but we can do better than this provided that we only have to _count_ the inversions and not _list_ them. 

Suppose for $n = 2^k$, we have:
* $c_{\ell}$ = number of inversions in the left half of the array
* $c_{r}$ = number of inversions in the right half of the array
* $c_t$ = number of **transverse** inversions with $i \leq \frac{n}{2}$ and $j > \frac{n}{2}$.

Our solution to this problem would then be $c_{\ell} + c_r + c_t$. We can find $c_{\ell}$ and $c_r$ recursively, but we need to find a way to calculate $c_t$.

We define a **transverse** inversion as two indices $(i, j)$ such that $i \leq \frac{n}{2}, j > \frac{n}{2}$, and $A[i] > A[j]$. For the first few recursive calls, we have:
$$
\begin{aligned}
c_{\ell} = 1 &\rightarrow (2, 3) \\\\
c_{r} = 3 &\rightarrow (6, 7), (6, 8), (7, 8) \\\\
c_{t} = 4 &\rightarrow (2, 5), (2, 8), (4, 5), (4, 8)
\end{aligned}
$$

It is easy to recursively calculate $c_{\ell}$ and $c_{r}$, but extra care is required for $c_{t}$. We want to find this in $\Theta(n)$ to ensure an algorithm better than $\Theta(n^2)$.

To calculate $c_{t}$, note that $c_{t}$ is simply the number of $i$s greater than all elements in the right half of the array. Alternatively, $c_{t}$ is simply the number all $j$ less than all elements in the left half of the array.

$$A = \[\color{red}1, 5, 2, 6 \color{black}, \color{blue} 3, 8, 7, 4 \color{black}\]$$

Since $c_t$ is not dependent on the ordering, we can sort the left half of the array and then for each $i$ in the left half, we apply binary search in the right half of the array. We would then have a recurrence relation $T(n) = 2T\left(\frac{n}{2}\right) + n\log(n)$, and unfortunately, we cannot apply the Master Theorem solve this. If we unwind the recursion:

$$
\begin{aligned}
    T(n) &= 2T\left(\frac{n}{2}\right) + n\log(n) \\\\
         &= 2\left(2T\left(\frac{n}{4}\right) + \frac{n}{2}\log\left(\frac{n}{2}\right)\right) + n\log n \\\\
         &= 4T\left(\frac{n}{4}\right) + n\log\frac{n}{2} + n\log n \\\\
         &\leq 4T\left(\frac{n}{4}\right) + 2n\log n \\\\
         &\leq 8T\left(\frac{n}{8}\right) + 3n\log n \\\\
         &\leq nT(1) + n (\log n)^2
\end{aligned}
$$
We can see that $T(n) \in \Theta(n\left(\log n\right)^2)$. This is still not ideal, however. An even better idea is to just modify the merge sort algorithm to **only** count the number of transverse inversions, recursively solving for $c_{\ell}$ and $c_r$, and calculating $c_t$ while merging.

This ideal solution will be in $\Theta(n \log n)$ just like Merge Sort. This will be a lower bound due to the comparison model.

### Finding Closest Pair of Points
> **Problem Statement:** \
> Given $n$ points $(x_i, y_i)$ in the plane, find a pair $(i, j)$ that minimizes the distance $d_{i, j} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$

We have a very trivial algorithm to find a pair in $O(n^2)$ time - simply try all pairs. However, we can use divide and conquer to find a better algorithm.

Consider sorting our points with respect to $x$ and $y$ first. Then, to divide the problem to smaller subproblems, consider finding the $x$-median and imagining some vertical line $L$ going across this point. Then, our closest pair of points can consist of either:

* Two points to the left of $L$ with minimum distance $\delta_L$
* Two points to the right of $L$ with minimum distance $\delta_R$
* One point to the left of $L$ and another to the right of $L$

Note that $\delta_L$ and $\delta_R$ are subproblems that can be determined recursively. So, we just need to find an algorithm to solve the third subproblem.

What we can do is filter out the points that have distance from $L$ greater than $\min\\\{\delta_L, \delta_R\\\}$. The resulting points will be present in a rectangular bound that when split into eight smaller squares will result in at most one point in each square. Then, we would have bounded the number of pairs to be checked by eight and we can do this in a linear manner since we have sorted our points. The proof of this bound is not provided, and tighter bounds appear to exist.

Sorting the points will be $O(n \log n)$ and the recombine step will be $\Theta(n)$. Our algorithm satisfies the recurrence relationship $T(n) = 2T\left(\frac{n}{2}\right) + \Theta(n)$. Therefore, we have $T(n) \in \Theta(n \log_2 n)$.



### Median of Medians
> **Problem Statement:** \
> Given $A$, find the entry that would be at index $\lfloor \frac{n}{2} \rfloor$ if $A$ was sorted.

This is a special case of the selection problem where we need to find the entry that would be at index $k$. We could sort to determine this, which would be $\Omega(n \log n)$. We could also use a randomized algorithm which has $O(n)$ **expected** running time. However, there exists a deterministic $O(n)$ algorithm to calculate the median, which will be presented.

As seen in previous courses, there is an efficient algorithm that solves the selection problem called QuickSelect. However, if the pivot selection is not ideal, we could have $O(n^2)$ time complexity. The question then becomes trying to find an ideal pivot - a pivot such that both $i$ and $n - i - 1$ are not too large, which will greatly improve the efficiency of the final algorithm. One such pivot is the median of medians. To calculate this, we can perform the following steps:

* Divide $A$ into $\frac{n}{5}$ groups $G_1, ... , G_{\frac{n}{5}}$ of size 5
* Find the medians $M = \[m_1, ... , m_{\frac{n}{5}}\]$
* Then, pivot $p$ is the median of $M$

Observe that in $M$, which has size $\frac{n}{5}$, half of them will be less than $p$, so there are $\frac{n}{10}$ elements in total that are less than $p$ that reside in $M$. For each of these $\frac{n}{10}$ numbers, there are at least 3 numbers that are greater or equal to $p$. Therefore, at least $\frac{3n}{10}$ numbers will be greater than $p$.

So, finding the medians of the medians and partitioning $A$ is $T\left(\frac{n}{5}\right) + O(n)$, and recursing on the appropriate side of $A$ will be at most $T\left(\frac{7n}{10}\right)$. Our final runtime for calculating the median then satisfies the relation:

$$T(n) \leq T\left(\frac{n}{5}\right) + O(n) + T\left(\frac{7n}{10}\right)$$

Not only will the Master Theorem **not** work, but the other strategies (loop unrolling, recursion trees) will be way too messy. Instead, let's propose that $T(n) \in O(n)$. A guess and check argument implies that:

$$T(n) \leq kn \implies T(n) \leq \frac{kn}{5} + \frac{7kn}{10} + \alpha n = \left(\alpha + \frac{9k}{10}\right)n$$

Then, we can take $k = 10\alpha$ to let $\alpha + \frac{9k}{10} = k$. We then see that $T(n) \in O(n)$, albeit with a very high constant.

Taking the median of five gives us optimal results - if we chose median of threes, then using a diagram, we can see that $T(n) \leq T\left(\frac{n}{3}\right) + T\left(\frac{2n}{3}\right) + O(n) \in O(n \log n)$.






 
























# Graph Algorithms

We will focus on both undirected and directed graphs. We wish to study breadth-first search and depth-first search and see its applications (shortest path, bipartite matching, cycle detection, cut vertices, etc.)

## Undirected Graphs

From MATH239, we know that a graph $G$ is a pair $(V, E)$ where:
* $V$ is a finite set of size $n$ whose elements are called vertices
* $E$ is a finite set of size $m$ whose elements are unordered pairs of distinct vertices. These are called the edges of $G$

To represent these using a data structure, we can use an adjacency list. This is an array $A[1..n]$ such that for all $i \in [1, n]$, $A[i]$ is the linked list (or any other collection) of all edges connected to $v$. It will have size $\Theta(n + m)$ and testing for edges will not be $O(1)$.

The definition of $G$ provided here generates an undirected graph where the edges do not have any "direction" and simply represent a connection.

Some key definitions to keep in mind:

1. **Path:** A sequence $v_1, ... , v_k$ of vertices with $(v_i, v_{i+1}) \in E$ for all $i$
2. **Connected Graph:** For all $v, w \in V$, there exists a path $v \to w$
3. **Cycle:** A path $v_1, ... , v_k, v_1$ with $k \geq 3$ and $v_i$'s pairwise distinct
4. **Tree:** A connected graph without any cycle
5. **Rooted Tree:** A tree with a special vertex called root
6. **Subgraph:** A graph $G' = (V', E')$ where $V' \subset V$ and $E' \subset E$ with all $e \in E'$ joining vertices $u, v \in V'$
7. **Connected Component:** A connected subgraph of $G$ not contained in a larger connected subgraph of $G$

## Breadth-First Search

### Main Idea

To perform a breadth-first search, we visit every node at the present depth prior to moving onto the nodes at the next depth. We can define depth as the distance away from $s$.

<center>
<img src="/Breadth-First-Tree-Traversal.gif" width="600" height="300" alt="BFS">
</center>


### Pseudocode
```a
G is a graph with n vertices implemented with an adjacency list
s is a vertex from G

BFS(G, s):
    let Q be an empty queue
    let visited be an array of size n with all entries set to false
    enqueue(s, Q)
    visited[s] = true
    while Q is not empty:
        v = dequeue(Q)
        for all w neighbours of v:
            if visited[w] is false:
                enqueue(w, Q)
                visited[w] = true
```

### Correctness + Time Complexity

To prove that this algorithm is correct, we first need to show that for all $v$, if $visited[v]$ is true at the end, then there exists a path $s \to v$ in $G$. A proof by induction will be used.

> For $i = 0$, this holds true. So then suppose true for $v_0, ..., v_{i - 1}$. When $v_i$ is set to true during the algorithm, we are looking at the neighbours of a certain $v_{j}$ that we already visited. By assumption, there is a path $s \to v_j$. Because $(v_j, v_i) \in E$, then there must exist a path $s \to v_i$.

We also need to prove that for all vertices $v$, if there is a path $s \to v$ in $G$, then $visited[v]$ is true at the end. A proof by induction will be used.

> Let $v_0 = s, ..., v_k = v$. We know that $visited[v_0]$ is true. So, then suppose $visited[v_i]$ is true. Then, we would examine all neighbours $v_j$ of $v_i$, of which we would set $visited[v_j] = true$ for each $v$. Note that this implies $visited[v_{i+1}]$ will be true, which then implies that $visited[v]$ will be true.

Therefore, in general, we have that for all vertices $v$, there is a path $s \to v$ if and only if $visited[v]$ is true at the end.

As for time complexity, note that each vertex is enqueued, dequeued, and read at most once. Therefore, will do a maximum of $|N|$ passes for the while loop. Then, using the convention that $d_v$ is the number of neighbours of $v$, we can apply the handshaking lemma to find the for loop is:

$$O\left(\sum_{v} d_v \right) = O(|E|)$$

This traversal alone will therefore have a cost $O(|V| + |E|)$.


### BFS Trees and Applications

The traversal of the edges done by BFS forms a BFS tree, which is a subgraph made up of all vertices $w$ such that $w$ has a parent and edges $\\\{w, parent[w]\\\}$. This traversal will give us the path from $s$ to $v$ if such a path exists. We can use the idea of BFS trees for many different problems.

To calculate the $parent$ array as well as the $level$ array (which can be useful for tracking distance), we can modify our BFS as such:

```a
G is a graph with n vertices implemented with an adjacency list
s is a vertex from G

BFS(G, s):
    let Q be an empty queue
    let visited be an array of size n with all entries set to false
    enqueue(s, Q)
    parent[s] = s
    level[s] = 0
    while Q is not empty:
        v = dequeue(Q)
        for all w neighbours of v:
            if visited[w] is false:
                enqueue(w, Q)
                parent[w] = v
                level[w] = level[v] + 1
```

We can then traverse through the $parent$ array to give us the path from $s$ to $v$.


#### Shortest Path via. BFS Trees

We can visualize the paths from $s$ to $v$ as traversing through a tree (more particularly, the BFS tree) where each level corresponds to the distance away from $s$. Then, we can intuitively see that BFS will naturally give us the shortest path. Observe that:

1. The levels in the queue are non-decreasing
2. For all vertices $u, v$, if there is an edge $(u, v)$, then $level[v] \leq level[u] + 1$

We can then conclude that for all $v$ in $G$, there is a path $s \to v$ in $G$ if and only if there is a path $s \to v$ in $T$. If so, then the path in $T$ is a shortest path and $level[v] = dist(s, v)$. We can print out the shortest path simply by traversing the $parent$ array if we wish.

#### Bipartite Testing via. BFS Trees

Recall that a graph $G = (V, E)$ is bipartite if there a partition $V = V_1 \cup V_2$ such that all edges $e \in E$ have one end in $V_1$ and one end in $V_2$.

<center>
<img src="/tree2.png" width="220" height="190" alt="Tree Bipartition">
</center>

Assuming that $G$ is connected, we can run BFS from any $s$ and set $V_1$ to be vertices with odd level and $V_2$ with vertices with even level. Afterwards, if there exists any edges between two vertices in $V_1$ or $V_2$, then $G$ is not bipartite. Otherwise, it is bipartite.



## Depth First Search 

### Main Idea

To perform a depth-first search, we select a node and travel as deep as possible, then backtrack (go to a previously visited node that still has children to visit) when we cannot go any further.

<center>
<img src="/Depth-First-Tree-Traversal.gif" width="520" height="300" alt="DFS">
</center>

### Pseudocode (Recursive)

```a
(Note: An iterative algorithm exists but it is not that important)

G is a graph with n vertices implemented with an adjacency list

DFS(G):
    for all v in G:
        if visited[v] is false
            explore(v)

explore(v):
    visited[v] = true
    for all w neighbour of v:
        if visited[w] = false:
            explore(w)
```

### Correctness + Time Complexity

To prove correctness of this algorithm, we need to prove the white path lemma - when we start exploring a vertex $v$, any $w$ that can be connected to $v$ by an unvisited path will be visited once $explore(v)$ is finished.

> Let $v_0 = v_1, ... , v_k = w$ be a path $v \to w$ with $v_1, ... , v_k$ not visited yet. For $i = 0$, this holds true. So then suppose true for $i < k$. When we visit $v_i$, $explore(v)$ is not finished and $v_{i + 1}$ is one of its neighbours. Either $visited[v_{i+1}]$ is true or we will visit it now, which means that it will be done before $explore(v)$ is finished.

Our BFS arguments allow for the rest of the required correctness proofs concerning paths. The runtime of DFS is the same as for BFS, which is $O(|V| + |E|)$.


### DFS Trees

Similar to BFS trees, the traversal of the edges done by DFS forms a DFS tree. It is beneficial to introduce the concept of a back edge, which is an edge in $G$ connecting an ancestor to a descendant which is not a edge in the DFS tree.


#### Start and Finish Times via. DFS Trees

One important thing we can do with DFS trees is record the time when a vertex is first visited and the time when its exploring is finished. To do this, we can simply modify our `explore()` function to be as such:

```a
start is an array of times when each vertex starts being explored
finish is an array of times when each vertex is done being explored

t = 1

explore(v):
    visited[v] = true
    start[v] = t
    t++
    for all w neighbours of v:
        if visited[w] = false:
            explore(w)
    finished[v] = t
    t++
```

Using the top-most vertex as the root, running DFS with this start/finish modification will yield the following, where $[x, y]$ represents the pair $[start[v], end[v]]$:

<center>
<img src="/cs341-start-finish.png" width="260" height="200" alt="Start Finish">
</center>

#### Cut Vertices via. DFS Trees

We say that $G$ is biconnected if $G$ is connected and $G$ stays connected if we remove any vertex (and all edges that contain it). If $G$ is not biconnected, then there exists a vertex $v$ such that removing it makes $G$ disconnected. This $v$ is known as a cut vertex (or articulation point).

Let's find an algorithm to find cut vertices. Our simplest case is that the root $s$ could be a cut vertex. This occurs if and only if it has more than one child. If $s$ has one child, then removing $s$ will leave $T$ connected. However, if $s$ has more than one child, then $s$ will have subtrees $S_1, S_k$, $k > 1$ and because there's no edge connecting $S_i$ to $S_j$, $i \neq j$, removing $s$ will create $k$ connected components.

Now let's consider the case where a non-root vertex is a cut vertex. Define:

* $a(v) = \min \\\{ \text{level}[w] \text{ such that } \\\{ v, w  \\\} \text{ is an edge} \\\}$
* $m(v) = \min \\\{ a(w) \text{ such that } w \text{ is a descendant of } v \\\}$

Finally, we state that any non-root vertex $v$ is a cut vertex if and only if it has a child $w$ with $m(w) \geq \text{level}[v]$. To see why, define $w$ as a child of $v$, $T_w$ be a subtree rooted at $w$, and $T_v$ be a subtree rooted at $v$. Then:

* If $m(w) < \text{level}[v]$, there is an edge from $T_w$ to a vertex above $v$, so $T_w$ would still be connected
* If $m(w) \geq \text{level}[v]$, all edges that originate from $T_w$ end in $T_v$, and because every edge originates from $x \in T_w$ and ends at a level at least $\text{level}[v]$, then removing $v$ will mean that $T_w$ is disconnected from the root.

This may not be so straightforward, so let's run through an example.

First, we can see that $a(v)$ is the highest (closest to root) that you can go from $v$ using a single edge from $v$ to $w$ where $w$ is some other vertex. Then, we can see that $m(v)$ is the highest that you can go from the subtree rooted at $v$ using a single edge. So, consider the following DFS tree:

<center>
<img src="/cs341-cut-vertices.png" width="260" height="200" alt="Start Finish">
</center>

We see that $a(v) = 1$ since the highest we can go only using the edges adjacent to $v$ is level one. We also see that $m(v) = 0$ since there exists a descendant of $v$ that is able to go up to the root (this is the bottom-left vertex, and it has $a(w) = 0$ since there exists an edge from that vertex to the root). Then, suppose we test whether or not $v$ is a cut vertex. The left child of $v$ will have $m(w_{\ell}) = 0$. However, the right child will have $m(w_{r}) = 2$. Therefore, there exists a child $w$ such that $m(w) \geq level[v] = 2$, thus $v$ is a cut vertex.

As for time complexity analysis, observe that: 
$$m(v) = \min \\\{ a(v), m(w_1), m(w_2), ... , m(w_k)\\\}$$
We can compute $a(v)$ in $O(|E|)$, all $m(w_i)$ in $O(d_v)$, and all $m(v)$ in $O(|E|)$. Furthermore, we can test the condition for $v$ in $O(d_v)$, so we can test all $v$ in $O(|E|)$. For our purposes, these details don't need to be known - it's sufficient to know that this algorithm is linear time. 

## Directed Graphs

### Basic Definitions

Up until this point, we have primarily focused on undirected graphs. Now, let's shift our attention to directed graphs. A directed graph is just like an undirected graph except with the following differences:

* The edges (which we can call arcs here) are directed pairs $(v, w)$ where the ordering of $v$ and $w$ matter. We call $v$ the source and $w$ the target.
* We can allow loops, so a cycle is any path $v_1, ..., v_k, v_1$ with $k \geq 1$. Note that this implies that paths can have length 1.

With these differences, we also have some new terminology to introduce:

1. **Reachable:** There exists a directed path from $v$ to $w$
2. **Directed Acyclic Graph:** A directed graph with no cycle
3. **Strongly Connected Component:** A component of $G$ such that for all $v, w$ in this component, $w$ is reachable from $v$ and $v$ is reachable from $w$

Our representation of graphs are nearly identical, and our algorithms for BFS and DFS are also nearly identical. For this course, we'll be concerned primarily with DFS. Before we get to the algorithms, we need to first introduce some terminology.

### DFS Forests w/ Directed Graphs

Extending the concept of DFS trees, we can have DFS forests that consist of multiple DFS trees. This is the case when a undirected graph is not connected, and as we will see now, whenever a directed graph is not strongly connected. We have four different type of edges in a DFS forest:

1. Tree edges (found during DFS)
2. Back edges (descendant to ancestor)
3. Forward edges (ancestor to descendant)
4. Cross edges (all other edges)

<center>
<img src="/cs341-dfs-forest.png" width="500" height="200" alt="DFS Forest">
</center>

We briefly discussed the following problems relating to directed graphs and various algorithms to solve them:

1. Observing that $G$ has a cycle if and only if there is a back edge in the DFS forest, we can test if $G$ is a directed acyclical graph by trying to find a back edge in $O(|V| + |E|)$.
2. Observing that $G$ is strongly connected if and only if for all $v, w$ in $G$, there is a $v-w$ path and a $w-v$ path, we can test if a graph is strongly connected by calling `explore()` twice - once using original edge directions and the other using reversed edge directions. We can do this in $O(|V| + |E|)$.
3. We can find all strongly connected components of a graph in $O(|V| + |E|)$ using Kosaraju's algorithm, which executes DFS twice based on start/finish times. We do not need to closely understand the details of this algorithm.










 








# Greedy Algorithms

## The Greedy Heuristic

We now introduce the idea of a greedy algorithm and see how it can be used to produce optimal solutions to certain problems involving costs. 

The idea of solving problems by selecting locally-optimal choices at each step in hopes of finding the global-optimal solution is known as the "greedy" problem-solving heuristic. An algorithm that follows this problem-solving strategy is known as a "greedy" algorithm.

There are only a few problems of which we can use a greedy algorithm to correctly solve it, and they need to meet the following conditions:

* The global-optimal solution can be found by using local-optimal choices at each step. In other words, we do not need to look ahead at each step
* The global-optimal solution contains optimal solutions to each of its subproblems

If a problem satisfies these two conditions, then it is often easy to come up with an efficient greedy algorithm, though proving its correctness tends to be very challenging. Otherwise, using a greedy algorithm can approximate at best and generate the worst possible solution at worst.

Let's investigate a wide variety of problems that can be solved with a greedy algorithm.

## Examples of Greedy Algorithms

### Job Scheduling

> **Problem Statement:** \
> Given $n$ jobs with processing times $t(1)..t(n)$, find the ordering of jobs that _minimizes_ the sum $T$ of completion times

To solve this problem using a greedy algorithm, all we need to do is sort the jobs in non-decreasing processing times. This will give us the optimal ordering and we can use an exchange argument to prove it:

1. Assume that in permutation $L = [e_1,  ... , e_n]$, there exists $i$ such that $t(e_i) > t(e_{i+1})$
2. Let $C$ be the completion time of jobs at $i-1$
3. Job $e_i$ has $C + t(e_i)$ completion time and job $e_{i+1}$ has $C + t(e_i) + t(e_{i+1})$ competion time. In total, we have $2C + 2t(e_{i}) + t(e_{i+1})$ completion time for all jobs up until $i+1$
4. Switch $e_i$ and $e_{i+1}$ to get a permutation $L'$
5. Now, we have $2C + 2t(e_{i+1}) + t(e_i)$ completion time for all jobs up until $i+1$
4. Since $t(e_{i}) > t(e_{i+1})$, the solution after the swap is better, so $L$ is not optimal

As a concrete example with $n = 3$ and processing times $[3, 6, 1]$:
* The order $[3, 6, 1]$ gives $T = 3 + (6 + 3) + (1 + 6 + 3) = 22$, which is not optimal!
* The order $[6, 1, 3]$ gives $T = 6 + (1 + 6) + (3 + 1 + 6) = 23$, which is not optimal!
* The order $[1, 3, 6]$ gives $T = 1 + (3 + 1) + (6 + 3 + 1) = 15$, which is optimal!


### Interval Scheduling

> **Problem Statement:** \
> Given $n$ intervals $I_1 = [s_1, f_1], ..., I_n = [s_n, f_n]$, find a subset of disjoint intervals with _maximized_ cardinality

To solve this problem using a greedy algorithm, all we need to do is create our subset $S$, sort the intervals by finish time (ascending) and while looping through the intervals, check if the current interval doesn't overlap the last inserted element of $S$, inserting the interval in $S$ if it doesn't.

<center>
<img src="/cs341-interval-scheduling.png" width="500" height="250" alt="Interval Scheduling">
</center>

Proof of correctness follows from an inductive proof that shows how any optimal solution cannot do worse than the greedy algorithm. Because we have to sort the intervals, our algorithm will be $O(n \log (n))$.

### Interval Colouring

> **Problem Statement:** \
> Given $n$ intervals $I_1 = [s_1, f_1], ..., I_n = [s_n, f_n]$, find a colouring of each interval such that overlapping intervals get different colours and the number of colours is _minimized_

To solve this problem using a greedy algorithm, all we need to do is sort the intervals by starting time (ascending) and while looping through the intervals, we use the minimum available colour $c_i$ to colour the current interval.

<center>
<img src="/cs341-interval-colouring.png" width="500" height="250" alt="Interval Colouring">
</center>

Proof of correctness follows from a proof that our algorithm cannot use less than $k$ colours. Because we have to sort the intervals, our algorithm will be $O(n \log (n))$.

### Fractional Knapsack

"Extra Content" - Consider doing this while studying for exams...

### Dijkstra's Algorithm

> **Problem Statement:** \
> Given a directed graph $G$ with non-negative weights $w(e)$ for each edge $e \in E$, determine the path from $s$ to $t$ with the _lowest_ weight.

Define $\delta(s, t)$ as the weight of the $s-t$ path with the least weight, with $\delta(s, t) = \infty$ if no path has been discovered. Set $\delta(s, s) = 0$. Observe that if $\delta(s, u) + w(u, v)$ is minimized, then we have $\delta(s, u) + w(u, v) = \delta(s, v)$. This gives us a way to determine the ideal path using a greedy algorithm. The particular algorithm that will be presented is Dijkstra's algorithm, and it is as follows:

1. Start at vertex $s$ and initialize $\delta(s, s) = 0$ and $\delta(s, x) = \infty$ for all other vertices $x$. Mark the source node as visited.
2. For each neighbouring vertex $v$ of the current vertex $u$, calculate $\delta(s, u) + w(u, v)$ - if this weight is less than the current $\delta(s, v)$, then update it to that. Then, from the set of unvisited vertices, select the node with the smallest distance and mark it as visited. Repeat this step until all nodes have been visited or $t$ has been visited.

<center>
<img src="/cs341-dijkstra.gif" width="400" height="300" alt="Dijkstra's Algorithm">
</center>

Since we are dealing with shortest paths, it is natural to perform some sort of breadth-first search, and this algorithm is quite similar to breadth first search. However, because the edges are weighted, we must use a priority queue as opposed to a regular queue. This will ensure that we have found the shortest distance to all vertices by the time we visit them and check its neighbours.

The time complexity of Dijkstra's algorithm is $O(|E| \log |V|)$ in practice.



### Kruskal's Algorithm

> **Problem Statement:** \
> Given a directed graph $G$ with weights $w(e)$ for each edge $e \in E$, determine a spanning tree with _minimum_ weight.

Recall that a spanning tree in $G$ is a tree of the form $(V, T)$ with $T \subset E$. Kruskal's algorithm gives us a greedy way to find a spanning tree of minimum weight:

1. Sort all the edges in the graph in non-decreasing order of their weight
2. Initialize an empty minimum spanning tree and a set of disjoint sets, each containing a single vertex
3. Iterate over the sorted edges in order, and for each edge, check if its two endpoints belong to different disjoint sets. If they do, add the edge to the minimum spanning tree and merge the two disjoint sets into a single set.
4. Continue until all edges have been processed or until the minimum spanning tree contains $|V|-1$ edges

<center>
<img src="/cs341-kruskal.gif" width="500" height="230" alt="Kruskal's Algorithm">
</center>

To implement this algorithm, we need a data structure that allows both find and union operations on disjoint sets of vertices. We need these operations to detect possible cycles, which our spanning tree does not permit. 

The time complexity of Kruskal's algorithm is $O(|E| \log |E|)$ in practice.
