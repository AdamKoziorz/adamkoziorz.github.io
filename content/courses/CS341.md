---
title: "CS341 - Algorithms"
url: "courses/CS341/"
summary: The study of efficient algorithms and effective algorithm design techniques
ShowToc: true
math: true
---

> **Instructor:** Eric Schost \
> **Lectures:** T/Th at 8:30am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 5 $\times$ 8% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 40% \
> \
> **Course Description:** \
> The study of efficient algorithms and effective algorithm design techniques. Program design with emphasis on pragmatic and mathematical aspects of program efficiency. Topics include divide and conquer algorithms, recurrences, greedy algorithms, dynamic programming, graph search and backtrack, problems without algorithms, NP-completeness and its implications.

<br/>

# Introduction

Building upon our CS240 knowledge of data structures and Big-O notation, as well as prior math, we will learn many algorithms (divide-and-conquer, BFS, DFS, greedy, dynamic programming, flows and cuts) and their pseudocode, correctness, and runtime. We will also briefly discuss NP-completeness and reductions.

## Prerequisite Knowledge

### Runtime Definitions
Recall that the input (problem instance) of an algorithm is parameterized by $n$ called the size. We then had the following brief definitions:

$T(I)$ - $\text{Runtime on input } I$

$T(n)$ - $\max_{I \text{ of size } n} T(I)$

$T_{avg}(n) - \frac{\sum_{I \text{ of size } n} T(I)}{\text{number of inputs of size } I}$

### Asymptotic Notation

Consider two functions $f(n)$, $g(n)$ with values in $\mathbb{R}_{>0}$. Then, we have:
* $f(n) \in O(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \leq Cg(n)$
* $f(n) \in \Omega(g(n))$ if $\exists C > 0$ and $\exists n_0$ such that for $n \geq n_0$, $f(n) \geq Cg(n)$
* $f(n) \in \Theta(g(n))$ if $f(n) \in O(g(n))$ and $f(n) \in \Omega(g(n))$
* $f(n) \in o(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) < Cg(n)$
* $f(n) \in \omega(g(n))$ if $\forall C > 0$, $\exists n_0$ such that for $n \geq n_0$, $f(n) > Cg(n)$

In the case of graphs and matrices, our algorithms may contain two parameters $n$ and $m$. We need to redefine our asymptotic notation to accommodate two parameters. For big-$O$ notation, we have the following definition:

$f(n, m) \in O(g(n, m))$ if $\exists C, n_0, m_0$ such that for $n \geq n_0$ or $m \geq m_0$, we have $f(n, m) \leq Cg(n, m)$

### Examples of Asymptotic Analysis
Here are some useful examples of asymptotic analysis that will be standard for the course going forwards:
* $n^k + c_{k-1}n^{k-1} + ... + c_0 \in \Theta(n^k)$
* $n^{O(1)}$ means (at most) polynomial in $n$
* $n \log(n) \in O(n^2), \Omega(n)$
* $2^{n-1} \in \Omega(2^n)$
* $(n-1)! \in o(n!)$ and NOT $\Theta(n!)$

Here are some practical notes for dealing with asymptotic analysis:
1. Big-$O$ is only an upper bound, and we should aim to give $\Theta$'s if possible.
2. Big-Anything hides constants and only states that one algorithm will _eventually_ beat another algorithm (ie. $\Theta(n^2)$ could be faster than $\Theta(n)$ for small enough input)
3. We will use a simplified model, focusing purely on "operations"

### Computational Model (word RAM)

Memory locations contain integer words of $b$ bits each, and $b \geq \log(n)$ for input size $n$.

Random Access Memory can access any memory location at unit cost, and all basic operations on words (loop counters, array indices, etc.) have unit costs. Unless otherwise implemented, we will always assume that these basic operations are constant time.

It is possible to perform run-time analysis in terms of integer words using the word RAM model, though we don't have to worry about this.

<br/>

# Divide and Conquer Algorithms

## Recurrences

Let's investigate Merge Sort, which uses divide and conquer to sort an array. With this sorting algorithm, we split the array into halves, sort those halves, then merge. Observe that merging requires at most $n - 1$ comparisons and can be done in-place using the two pointers strategy. Merge sort follows this recurrence relation:

$$ t(1) = 0, \hspace{0.4cm} t(n) = 2t\left(\frac{n}{2}\right) + n - 1$$

The $n - 1$ is not very convenient to analyze, so let's instead define:

$$ T(1) = 0, \hspace{0.4cm} T(n) = 2T\left(\frac{n}{2}\right) + n$$

and observe that $t(n) \leq T(n)$ by induction.

Let's unroll the recurrence relation:

$$
\begin{aligned}
    T(n) &= \color{red}{2} \color{black}T\left(\frac{n}{\color{red}{2} \color{black}}\right) + \color{red}{1} \color{black}n \\\\
         &= 2\left(2T\left(\frac{n}{4}\right) + \frac{n}{2}\right) + n \\\\
         &= \color{red}{4} \color{black}T\left(\frac{n}{\color{red}{4} \color{black}}\right) + \color{red}{2} \color{black}n \\\\
         &= \color{red}{8} \color{black}T\left(\frac{n}{\color{red}{8} \color{black}}\right) + \color{red}{3} \color{black}n \\\\
         &= ... \\\\
         &= \color{red}{n} \color{black}T\left(\frac{n}{\color{red}{n} \color{black}}\right) + \color{red}{\log_{2}(n)} \color{black} n \\\\
         &= nT(1) + n\log_{2}(n) \\\\
         &= n\log_{2}(n) \\\\
\end{aligned}
$$

Thus, we can see that merge sort is $O(n \log_{2}(n))$. If you look at the recursion tree for Merge Sort below, we get the same result:

![Recursion-Tree](/recursion-tree-example.png)

This recursion tree demonstrates how the problems are split up until we get $\frac{n}{2^k} = 1$ and how we add up the work on each level. The tree here has height $\log_2(n)$ and on each level, we perform a $O(n)$ operation, so the total work done is $O(\log_2(n))$.

Note that with Merge Sort, this is actually a tight bound, so Merge Sort has $\Theta(n\log_{2}(n))$ time complexity. The question then becomes, how do we handle more complicated recurrence relations?

## Master Theorem

Consider two integers $a, b \geq 1$, a real number $c \geq 0$, and a function $T(n)$ such that:

$$ T(1) = C, \hspace{0.4cm} T(n) = aT\left(\frac{n}{b}\right) + \Theta(n^c)$$

where $n$ is a power of $b$. Observe that:
* $b$ is the factor by which we reduce the problem size
* $a$ is the number of recursive calls
* $\Theta(n^c)$ is the cost to prepare the recursive calls and combine their results

A simplified version of the master theorem states that:

$$T(n) \in \begin{cases}
    \color{red} \Theta(n^c) \color{black} & \text{if } c > \log_{b}(a) \hspace{0.6cm} \text{(root-heavy)} \\\\
    \color{green} \Theta(n^c \log(n)) \color{black} & \text{if } c = \log_{b}(a) \hspace{0.6cm} \text{(balanced)} \\\\
    \color{blue} \Theta(n^{\log_{b}(a)}) \color{black} & \text{if } c < \log_{b}(a) \hspace{0.6cm} \text{(leaf-heavy)}
    \end{cases} $$

Observe that there is $1$ root, $a$ children, $a^2$ grandchildren, and $a^k$ leaves.

We can argue that this works in cases where our subproblems may not have exact sizes (ie. in the case of floors/ceilings), allowing us to perform "lazy" evaluations. The proof for this fact is out of scope for this course.

#### Proof

A cleaner version of this theorem has $b^k$ in replacement of $n$. We can prove that $n = b^k$ using recursion unrolling, logarithm rules, and case analysis. Observe that:

$$
\begin{aligned}
T(b^k) &= aT(b^{k-1}) + b^{kc} \\\\
       &= a(aT(b^{k-2}) + b^{(k-1)c}) + b^{kc} \\\\
       &= a^2T(b^{k-2}) + ab^{(k-1)c} + b^{kc} \\\\
       &= ... \\\\
       &= a^{k} + a^{k-1}b^{c} + a^{k-2}b^{2c} + ... + ab^{(k-1)c} + b^{kc} \\\\
       &= a^k \left(1 + \frac{b^c}{a} + ... + \left(\frac{b^c}{a}\right)^k\right) \\\\
       &= (b^c)^k \left(1 + \frac{a}{b^c} + ... + \left(\frac{a}{b^c}\right)^k \right)
\end{aligned}
$$

For $n = b^k$, we have $k = \log_{b}(n)$, so $a^k = a^{\log_b(n)} = n^{\log_{b}(a)}$. Then, observe that:

* If $a = b^c$, then from the second last summation, we can successfully onclude that $T(n) = a^k (k + 1) = \color{green} n^c(\log_{b}(n) + 1)$
* If $a > b^c$, then we have a geometric series evident in the second last summation, which gives $a^k \leq T(n) \leq K_1 a^k = n^{\log_{b}(a)} \leq T(n) \leq K_1 \color{red} n^{\log_{b}(a)}$
* If $a < b^c$, then we have a geometric series evident in the last summation, which gives $(b^c)^k \leq T(n) \leq K_2 (b^c)^k = n^c \leq T(n) \leq K_2 \color{blue} n^c$

Observe that these only hold in the case where $n = b^k$.

#### Alternatives to Master Theorem

If we cannot apply the Master Theorem to a chosen recurrence, then we need to either use a recursion tree or unroll the recurrence. To use the recursion tree argument:

1. Draw the tree based on the recurrence given - the tree's leaves should all be $T(1)$
2. For each level of the tree, identify the size of the subproblems, number of nodes, cost/node, and total cost. It is helpful to identify common patterns in the numbers like shown with the Merge Sort example
3. Sum the costs of all levels of the recursion tree and determine the order

## Divide and Conquer

To solve a problem in size $n$ using the **Divide and Conquer** strategy, we:

1. **Divide**: Break the input into smaller problems, ideally into size $\frac{n}{b}$
2. **Conquer**: Solve these subproblems recursively
3. **Recombine**: Deduce the solution of the main problem from the subproblems

For the remainder of this section, we will see how we can solve some problems using the Divide and Conquer approach.

### Maximum Subarray (via. Divide and Conquer)

> **Problem Statement:** \
> Given an array $A[0..n-1]$, find a contiguous subarray $A[i..j]$ that maximizes the sum $A[i] + A[i+1] + ... + A[j-1] + A[j]$ and return this sum. \
> This corresponds to the "Maximum Subarray" problem on [LeetCode](https://leetcode.com/problems/maximum-subarray/).

A brute force solution to this problem is $\Theta(n^2)$ and consists of checking every range for the maximum sum. Instead, let's solve this problem using Divide and Conquer.

Here is our setup:

1. **Divide** the problem of size $n$ into two subproblems of size $\frac{n}{2}$ (the lower half and upper half of the array)
2. **Conquer** these subproblems recursively, determining the base case
3. **Recombine** the subproblems to determine our solution

We need to then carefully analyze step 3 - how do we actually use the halfs of the array to determine the solution? Observe that there are three cases here:
1. The maximum subarray lies in the lower half
2. The maximum subarray lies in the upper half
3. The maximum subarray starts in the lower half and ends in the upper half

In the third case, consider maximizing the lower and upper halves independently (get the maximum sum of these halves). Here is the solution in pseudocode.
```cpp
int maxSubArray(vector<int>& nums) {                  // Wrapper Function
    return maxSubArray(nums, 0, nums.size() - 1);
}

int maxSubArray(vector<int>& nums, int l, int u) {    // The Main Algorithm
    if (l == u) return nums[l];                       // Base Case

    int m = l + (u - l) / 2;                          // Calculate Middle

    int lmax = nums[m];                               // Get maximum sum of lower half
    for (int i = m, sum = 0; i >= l; --i) {           // O(n)
        sum += nums[i];
        if (sum > lmax) lmax = sum;
    }

    int rmax = nums[m + 1];                           // Get maximum sum of upper half
    for (int i = m + 1, sum = 0; i <= u; ++i) {       // O(n)
        sum += nums[i];
        if (sum > rmax) rmax = sum;
    }

    return max({ maxSubArray(nums, l, m),             // Divide, Conquer, Recombine
                 maxSubArray(nums, m + 1, u),         // T(n) = 2T(n/2)
                 lmax + rmax });                      //       (+ O(n) from before)
}
```
Now with this approach, we improve our time complexity to $\Theta(n \log_{2} n)$, as proven by applying the Master Theorem to the recurrence relation $T(n) = 2T(\frac{n}{2}) + \Theta(n)$. Observe that this is the same recurrence relation as Merge Sort.

For this problem specifically, there is actually a better solution to this that involves dynamic programming, and we will eventually get to it

### Karatsuba's Algorithm

> **Problem Statement:** \
> Given two polynomials $F = f_0 + ... + f_{n-1}x^{n-1}$ and $G = g_0 + ... + g_{n-1}x^{n-1}$, complete their product $FG$, given by: \
> $$ H = FG = f_0g_0 + (f_0g_1 + f_1g_0)x + ... + f_{n-1}g_{n-1}x^{2n-2} $$

As a prelimary, we will assume that $\forall i, |f_{i}| \leq K$ and $|g_{i}| \leq K$. Then, $|h_{i}| \leq nK^2$. In the word RAM model where the input size is $2^b$, the output requires at most $3b$ words in this model, which is $O(1)$. However, for our assignment where $K = 10^5$ and $n \leq 10^5$, every coefficient in the output will fit in 128 bits.

An easy brute force algorithm in $\Theta(n^2)$ would look like:
```
for i = 0 to n-1 do:
    for j = 0 to n-1 do:
        h{i + j} = h{i + j} + f{i}g{j}
```

However, Karatsuba provides a Divide and Conquer algorithm that allows the multiplication of polynomials in less than $\Theta(n^2)$.

#### The Algorithm

Suppose we cut a polynomial in half like this:
$$F(x) = 1 + 2x + 3x^2 + 4x^3 = 1 + 2x + x^2(3 + 4x)$$

More generally, we write $F = F_0 + F_1x^{\frac{n}{2}}$ and $G = G_0 + G_1x^{\frac{n}{2}}$. Then, $H$ can look like:

$$ H = F_0G_0 + (F_0G_1 + F_1G_0)x^{\frac{n}{2}} + F_1G_1x^n $$

Observe that we have 4 recursive calls in size $\frac{n}{2}$, and we have $\Theta(n)$ additions to compute $F_0G_1 + F_1G_0$ as well as the overlaps. We can use the following identity to adjust this to only 3 recursive calls coloured in red, green, and blue respectively.

$$
\begin{aligned}
& \hspace{0.2cm} (F_0 + F_1x^{\frac{n}{2}}) (G_0 + G_1x^{\frac{n}{2}}) \\\\
=& \hspace{0.2cm} \color{red} F_0 G_0 \color{black} + (\color{green} (F_0 + F_1)(G_0 + G_1) \color{black} - \color{red} F_0 G_0 \color{black} - \color{blue} F_1 G_1) \color{black} x^{\frac{n}{2}} + \color{blue} F_1 G_1 \color{black} x^n
\end{aligned}
$$

Our recurrence relation is then $T(n) = 3T\left(\frac{n}{2}\right) + \Theta(n)$. Applying the master theorem gives $T(n) \in \Theta(n^{\log_{2}(3)})$.

#### Example

Let's rewrite our polynomials in the specified form:
$$ F(x) = 1 + 2x + 3x^2 = 1 + 2x + x^2(3) $$
$$ G(x) = 3 + 4x + 5x^2 = 3 + 4x + x^2(5) $$

Note that $n = 3$, $F_0 = 1 + 2x$, $F_1 = 3$, $G_0 = 3 + 4x$, and $G_1 = 5$. 
Therefore, our subproblems are as follows:

* $\color{red} F_0G_0 \color{black} = 3 + 10x + 8x^2$ 
* $\color{blue} F_1G_1 \color{black} = 15$
* $\color{green} (F_0 + F_1)(G_0 + G_1) \color{black} = 32 + 32x + 8x^2$

Which applying the identity gives:

$$
\begin{aligned}
H(x) &= (3 + 10x + 8x^2) + ((32 + 32x + 8x^2) - (3 + 10x + 8x^2) - 15))x^2 + 15x^4 \\\\
&= 3 + 10x + 8x^2 + (14 + 22x)x^2 + 15x^4 \\\\
&= 3 + 10x + 22x^2 + 22x^3 + 15x^4
\end{aligned}
$$


### Strassen's Algorithm

> **Problem Statement:** \
> Given two matrices $A = \[a_{i, j}\]$ and $B = \[b_{j, k}\]$, compute $C = AB$

Let $A$ and $B$ be 2-dimensional matrices, and observe that:
$$
C = 
\begin{pmatrix}
    A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2, 2} \\\\
    A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{2,2} + A_{2,2}B_{2, 2}
\end{pmatrix}
$$

With the brute force algorithm, we would have 8 recursive calls in size $\frac{n}{2}$ with $\Theta(n^2)$ additions. Strassen's algorithm allows us to calculate $C$ using only 7 recursive calls, giving a new time complexity of $T(n) \in \Theta(n^{\log_{2}(7)})$ in the case of two dimensional matrices. The exact details of the algorithm do not need to be memorized, and thus won't be presented. However, the algorithm itself is discussed because it shows another way you can utilize mathematical properties (like in the Karatsuba algorithm) to shorten the number of recursive calls.

It is good to keep note however that for matrix multiplication, any algorithm that does $k$ multiplications for matrices of size $\ell$ will always be $T(n) \in \Theta(n^{\log_{\ell}(k)})$.


### Counting Inversions

> **Problem Statement:** \
> Given an unsorted array $A$, find the number of inversions in it. Two indices $(i, j)$ is an inversion if $i < j$ and $A[i] > A[j]$.

As a guiding example, note that with $A = \[1, 5, 2, 6, 3, 8, 7, 4\]$:
$$|(2, 3), (2, 5), (2, 8), (4, 5), (4, 8), (6, 7), (6, 8), (7, 8)| = 8$$

We have an easy algorithm in $\Theta(n^2)$ to do this, but we can do better than this provided that we only have to _count_ the inversions and not _list_ them. 

Suppose for $n = 2^k$, we have:
* $c_{\ell}$ = number of inversions in the left half of the array
* $c_{r}$ = number of inversions in the right half of the array
* $c_t$ = number of **transverse** inversions with $i \leq \frac{n}{2}$ and $j > \frac{n}{2}$.

For the first few recursive calls, we have:
$$
\begin{aligned}
c_{\ell} = 1 &\rightarrow (2, 3) \\\\
c_{r} = 3 &\rightarrow (6, 7), (6, 8), (7, 8) \\\\
c_{t} = 4 &\rightarrow (2, 5), (2, 8), (4, 5), (4, 8)
\end{aligned}
$$

It is easy to recursively calculate $c_{\ell}$ and $c_{r}$, but extra care is required for $c_{t}$. We want to find this in $\Theta(n)$ to ensure an algorithm better than $\Theta(n^2)$.

To calculate $c_{t}$, note that $c_{t}$ is simply the number of $i$s greater than all elements in the right half of the array. Alternatively, $c_{t}$ is simply the number all $j$ less than all elements in the left half of the array.

$$A = \[\color{red}1, 5, 2, 6 \color{black}, \color{blue} 3, 8, 7, 4 \color{black}\]$$

Since $c_t$ is not dependent on the ordering, we can sort the left half of the array and then for each $i$ in the left half, we apply binary search in the right half of the array.

We have a recurrence relation $T(n) = 2T\left(\frac{n}{2}\right) + n\log(n)$, and unfortunately, we cannot apply the Master Theorem to solve this. If we unwind the recursion:

$$
\begin{aligned}
    T(n) &= 2T\left(\frac{n}{2}\right) + n\log(n) \\\\
         &= 2\left(2T\frac{n}{4} + \frac{n}{2}\log\left(\frac{n}{2}\right)\right) + n\log n \\\\
         &= 4T\left(\frac{n}{4}\right) + n\log\frac{n}{2} + n\log n \\\\
         &\leq 4T\left(\frac{n}{4}\right) + 2n\log n \\\\
         &\leq 8T\left(\frac{n}{8}\right) + 3n\log n \\\\
         &\leq nT(1) + n (\log n)^2
\end{aligned}
$$
We can see that $T(n) \in O(n\left(\log n\right)^2)$. This is still not ideal, however. An even better idea is to perform Merge Sort **while** performing the recursive calls for $c_{\ell}$ and $c_{r}$.

```
Merge(A)
    copy A to new array S; c = 0;
    i = 1; j = n/2 + 1;
    for (k = 1; k <= n; k++) do
        if (j > n/2) then A[k] = S[j++];
        else if (j > n) then
            A[k] = S[i++];
            c = c + n/2;
        else if (S[i] < S[j]) then
            A[k] = S[i++];
            c = c + j - (n/2 + 1);
        else A[k] = S[j++];
```

Then, this is just a modification of the Merge Sort algorithm and our final solution will be in $\Theta(n \log n)$. Keep in mind that $\Omega(n \log(n))$ is a lower bound as we have to perform comparisons.

### Finding Closest Pair of Points
> **Problem Statement:** \
> Given $n$ points $(x_i, y_i)$ in the plane, find a pair $(i, j)$ that minimizes the distance.









### Median of Medians
> **Problem Statement:** \
> Given $A$, find the entry that would be at index $\lfloor \frac{n}{2} \rfloor$ if $A$ was sorted.

This is a variant of the selection problem where we need to find the entry that would be at $k$. We could sort to determine this or use a randomized algorithm which depends on luck.

However, there is a linear algorithm to the selection problem called QuickSelect:

```cpp
int partition(int arr[], int l, int r) {
    int x = arr[r], i = l;
    for (int j = l; j <= r - 1; j++) {
        if (arr[j] <= x) {
            swap(arr[i], arr[j]);
            i++;
        }
    }
    swap(arr[i], arr[r]);
    return i;
}

int quickSelect(int arr[], int l, int r, int k) {
    if (k > 0 && k <= r - l + 1) {
        int index = partition(arr, l, r);
  
        if (index - l == k - 1)
            return arr[index];
  
        if (index - l > k - 1) 
            return quickSelect(arr, l, index - 1, k);
  
        return quickSelect(arr, index + 1, r, 
                            k - index + l - 1);
    }
    return INT_MAX;
}
```

The question then becomes trying to find a pivot such that both $i$ and $n - i - 1$ are not too large. To do this:
* Divide $A$ into $\frac{n}{5}$ groups $G_1, ... , G_{\frac{n}{5}}$ of size 5
* Find the medians $m_1, ... , m_{\frac{n}{5}}$ of each group
* pivot $p$ is the median of $\[m_1, ... , m_{\frac{n}{5}}\]$

With this choice of $p$, our indices $i$ and $n - i - 1$ will be at most $\frac{7n}{10}$. This follows from the fact there are 3 elements in $G_i$ greater than or equal to $m_{i}$.

Our runtime then satisfies:

$$T(n) \leq T\left(\frac{n}{5}\right) + T\left(\frac{7n}{10}\right) + O(n)$$

Not only will the Master Theorem **not** work, but the other strategies (loop unrolling, recursion trees) will be way too messy. Instead, let's propose that $T(n) \in O(n)$. A guess and check argument implies that:

$$T(n) \leq kn \implies T(n) \leq \frac{kn}{5} + \frac{7kn}{10} + \alpha n = \left(\alpha + \frac{9k}{10}\right)n$$

Then, we can take $k = 10\alpha$ to let $\alpha + \frac{9k}{10} = k$. We then see that $T(n) \in O(n)$, albeit with a very high constant.


