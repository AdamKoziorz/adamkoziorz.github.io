---
title: "CS350 - Operating Systems"
url: "courses/CS350/"
summary: An introduction to the fundamentals of operating system function, design, and implementation
ShowToc: true
math: true
---

> **Instructor:** Rob Hackman \
> **Lectures:** T/Th at 10:00am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 2%, 12%, 11%, 15% \
> Reading Assignments: 2 $\times$ 5% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 30% \
> \
> **Course Description:** \
> An introduction to the fundamentals of operating system function, design, and implementation. Topics include concurrency, synchronization, processes, threads, scheduling, memory management, file systems, device management, and security.

 

# Introduction

## Motivation
How do multiple programs run at the same time and utilize computer resources? The "middle man" responsible for this is the Operating System (OS). This is a maturing field and many different computer science topics are OS issues (high-performance servers, resource consumption, security, databases, game engines, etc.)

We will learn the basics of operating systems, what they do, what issues they can face, and how they affect software. Some key concepts include multi-programming, concurrency, memory management, device management, and file systems. We will focus on these concepts with an emphasis on security and protection. 


## What is an Operating System?

The operating system (OS) is a layer between applications and hardware that provides an **abstraction** to the hardware details and program resources, allowing programmers to interact with hardware securely and effectively. It contains a kernel that responds to system calls, interrupts, and exceptions.

Originally, they were just a library of standard services that didn't really offer any security and the system could only run one program at a time. However, 
multitasking capabilities were then realized to make it appear as though multiple programs are running at the same time to allow for concurrency. More features were eventually added, leading to a prototype of a modern OS.

### Views of an OS

1. **Application View**:

    The OS provides an execution environment for running programs. This environment provides us with an interface to access networks, storage, I/O devices, as well as enables programs to run alongside other programs with the appropriate protections.

2. **System View**:

    The OS manages the hardware resources of a computer system. It allocates the required resources for programs to run and controls how they are shared. Such resources includes processors, memory, disks, storage devices, networks, and I/O devices.

3. **Implementation View**:

    The OS is a concurrent, real-time program where it needs to run alongside other programs and must respond to events with specific timing constraints.

### How does the OS protect us?

1. **Preemption**:

    Give applications a resource, take it away if needed elsewhere

2. **Interposition**:

     Place the OS between application and "stuff", track all pieces that application allowed to use, and on every access, look in table to check that access legal

3. **(Un)privileged Modes**:

     Applications are unprivileged and the OS is privileged - system calls can only be done in privileged mode

### System Calls

If we have to perform system calls (unprivileged to privileged), then special instructions will invoke a `syscall` handler which stores the state of the registers, loads into the program counter a predetermined address in the kernel memory (enabling privileged mode), then returns back to the original application (disabling privileged mode).

We want to perform system calls if our application needs to do things that it cannot do in unprivileged mode. Higher-level functions such as `printf`, `gets`, `scanf`, etc. are built upon the `syscall` interface.

### Schematic of the OS

Below, we have a schematic of the OS:

<center>
<img src="/cs350-os-example.png" width="550" height="350" alt="OS Schematic">
</center>

 Note that the user programs here would be unprivileged, and that the program needs to interact with the kernel in order to run in privileged mode and access hardware.








 

# Processes

The OS provides processes, threads, locks, and file I/O to applications. Let's focus on the process abstraction first.

## Introduction

**Process:** An instance of a running program

**Thread:** A sequence of scheduled executions (we will discuss this more later)

The OS keeps track of processes using a data structure which includes execution details such as running threads, address space, open files, and more. Each process has their own unique process ID (PID) and will always have at least one thread (the program entry point).

The OS enables multiple processes to run, increasing CPU utilization and reducing latency. Most programs do not care which other programs are running, but not all programs (we will see how to deal with this later).

Here are the states of a process/thread:

* Running $\rightarrow$ Blocked: Asks for a resource that is not available
* Blocked $\rightarrow$  Ready: Resource becomes available
* Ready $\rightarrow$  Running: When the OS schedules it (core is available)
* Running $\rightarrow$  Ready: Yielding or preemption

<center>
<img src="/cs350-process-states.png" width="550" height="250" alt="Process States">
</center>

Whenever a thread stalls for input, another thread will execute on the CPU core until input has been received (moving the thread into a ready state) AND the other thread has been kicked off by the OS. A nice real-life analogy to this is sharing equipment with someone on an fixed ON/OFF cycle.


## User View of Processes

### `int fork (void)`:

1. **Description**:

    Creates a new process that is a (near) exact copy of the current one

2. **Return Values**:

    - `pid` of the new process in "parent" process after the fork
    - `0` in the "child" process after the fork

3. **Example Usage**:

    ```c
    #include <unistd.h>
    #include <sys/wait.h>

    switch (pid = fork()) {    // we check the return value of fork()
        case -1:
            perror("fork"); break;
        case 0:
            // do something in child process
        default:
            // do something in parent process
    }
    ```

4. **Other Notes**:

    We would like to use forks in cases where we might want to have parallelism (Nginx, PostgreSQL, etc.). It is very simple to deal with as there are no arguments, and we can use the child to perform any different tasks like manipulate file descriptors, environment, and set resource limits. Note that the processes are independent, so they have their own memory.  

 

### `int waitpid (int pid, int *stat, int opt)`:

1. **Description**:

    Suspends execution of the calling process until the process indicated by `pid` has changed state (ends or is stopped)

2. **Arguments**:

    - `pid`: Process to wait for, or -1 for any
    - `stat`: The exit value or signal
    - `opt`: Usually `0` or `WNOHANG`

3. **Return Values**:

    - `pid` if successful, otherwise `-1`

4. **Example Usage**:

    ```c
    #include <unistd.h>
    #include <stdio.h>
    #include <sys/wait.h>

    int main() {
        int rc1, rc2, rc3;
        rc1 = fork();
        rc2 = fork();
        rc3 = fork();
        int exit_status;
        printf("1");                      // This gets printed 8 times!
        waitpid(rc1, &exit_status, 0);
        waitpid(rc2, &exit_status, 0);
        waitpid(rc3, &exit_status, 0);
    }
    ```

5. **Additional Notes**:

    - We use this function if and only if we need to wait for a *child* process to finish execution to continue executing. This should always be used if a process is being forked so all children processes are finished before the parent process can finish.

    - If a process exits but its parent is still alive, then we must store the exit code status somehow. The best way to approach this is to destroy its information but leave the data structures available and leave an exit code.

    - You can only free processes when waitpid is called on it or when its parent exits (you need to check all the children in this case).

    - If process A creates process B and process A exits, then process B should **not** be killed. However, when process B returns, then it needs to check if its parents is alive, and if not, then process B is responsible for cleaning itself up.


 

### `void exit (int status)`:

$\hspace{0.5cm}$ Closes the current process and passes `status` to its parent (usually through `waitpid`) for it to use as it wishes.

 

### `int kill (int pid, int sig)`:

1. **Description**:

    Sends `sig` to a process `pid` to forcibly interrupt that process

2. **Arguments**:

    - `pid`: The pid of the process we want to interrupt
    - `sig`: The signal we wish to send to the process

3. **Return Values**:

    - `0` if successful, otherwise `-1`


 


### `int execve(char *prog, char **argv, char **envp)`:

1. **Description**:

    Executes a specified program (`prog`) and passes the provided command-line arguments and environment variables.

2. **Arguments**:

    - `prog`: Full pathname of program to run
    - `argv`: Argument vector that gets passed to main
    - `envp`: Environment variables (eg. `PATH`, `HOME`)

3. **Return Values**:

    - `execve` should not return any values

4. **Example Usage**:

    ```c
    #include <sys/wait.h>
    #include <errno>

    int main(int argc, char **argv) {
        int rc = fork();
        if (rc == 0) {
            char *args[] = {"grep", argv[1], "alice.txt", NULL};
            execvp("grep", args);
            printf("Execv error code: %d\n", x);
            printf("Execv errno: %d\n", errno);
        }
        int status;
        waitpid(rc, &status, 0);
        if (WEXITSTATUS(status) == 0) {
            printf("Word %s was found \n", argv[1]);
        } else {
            printf("Word %s was not found \n", argv[1]);
        }
    }
    ```

5. **Additional Notes**:

    - This does not create a new process - it changes the identity of the program instead. Therefore, it should only be used on a child process
    - There are variants of this function (`execvp` and `exevlp`) that have similar functionality to `execve` but with some small differences - these will not be covered individually

 

### `int dup2 (int oldfd, int newfd)`:

This command will close `newfd` if it was a valid file descriptor and makes `newfd1` an exact copy of `oldfd`. Two file descriptors will share the same offset.

Note that `dup2(x, 0)` will change `x`'s file descriptor to `stdin` and `dup2(x, 1)` will change `x`'s file descriptor to `stdout`.

### `int fcntl (int fd, F_SETFD, int val)`:

This will set the `close on exec` if `val = 1` and clears if `val = 0`. The file descriptor will be made non-inheritable by the new program.

### `int pipe (int fds[2])`:

This will return two file descriptors in `fds[0]` and `fds[1]`. Any writes to `fds[1]` will be read on `fds[0]`, and when the last copy of `fds[1]` is closed, then `fds[0]` will return `EOF`. It will return `0` on success and `-1` on error.

Pipes can read, write, and close just like with files. If `fds[1]` is closed, then `read(fds[0])` will return 0 bytes. When `fds[0]` is closed, then `write(fds[1])` and we kill the process.


 

## Kernel View of Processes

### Implementation

The OS keeps a data structure for each proc (Process Control Block). It will track the information necessary for each process to run, which includes:
* Process State and Process ID
* Registers and Program Counter
* Address Space
* Open Files

If a process is forked, the parent and child processes will share the same address space, but they will not share memory.

### User and Kernel Modes

A key component of an operating system is to introduce restrictions on what a process can do for safety purposes (I/O, device connection, create new processes, etc.). The way the OS does this is to have a **user mode** (unprivileged) and a **kernel mode** (privileged).

Because our own programs may need to execute privileged instructions, the kernel will provide the ability to perform system calls.

How this works is that the program will execute a `trap` instruction which sends a signal to the OS to switch to kernel mode and executes the appropriate kernel code.  Then once this kernel code is executed, the OS calls a `return-from-trap` instruction that returns to user mode and continues executing the original program.

To ensure that the proper code is executed, the kernel needs to set up a trap table that tells the hardware what code to run when certain signals are received.

<center>
<img src="/cs350-trap-example.png" width="500" height="600" alt="Trap Example">
</center>





### Preemption

We can preempt (interrupt) a process when the kernel gets control. If there is such an interrupt (scheduling quantum, device recognition, etc.), then we need to save the state of the process/thread before the kernel can run its code. This is what is known as context-switching. We can use a switch statement to understand the content of the interrupt, of which the appropriate kernel code will be executed.
\
The CPU has a clock of which we can set up a timer to perform interrupts.

Whenever a process is interrupted, it will save:
1. The program counter and integer registers (always)
2. Special registers and floating points
3. Condition codes

<center>
<img src="/cs350-switch-example.png" width="500" height="500" alt="Switch Example">
</center>


then possibly change virtual address translations. We can then see that context switching is expensive and can cause TLB flushes as well as cache misses.

In OS161, if process 1 gets preempted, then the following calls occur on its stack:

Thread data $\to$ Trap Frame $\to$ MIPS trap (general) $\to$ mainbus_interrupt (specific) $\to$ thread_switch $\to$ switchframe_switch

Switch frames are just the special registers, and when we call `switchframe_switch`, its returns into thread_switch in process 2's stack (we switch threads in the middle of the execution of switchframe_switch). Implementation is found in `switch.S`.



### Scheduling

* If 0 threads are runnable, then halt CPU
* If 1 thread is runnable, then run the thread
* If 2+ threads are runnable, make a scheduling decision.  We could scan the process table, use FIFO/Round-Robin (this is used in OS161) or use priorities.

To demonstrate FIFO, consider thread $A$, $B$, and $C$ (in this order) and suppose $A$ gets to run until it is finished. According to this system, $B$ then gets to run until it is finished, then $C$ gets to run until it is finished.

Round-Robin is just FIFO but with preemption introduced. So, $A$ runs for a bit then is kicked out. Then, $A$ is placed at the end of the queue and $B$ runs for a bit. Then, $B$ is kicked out, placed at the end of the queue, and $C$ runs for a bit. Now, when $C$ is kicked off, we will run $A$.



















 

# Threads

Now, we will focus on the thread abstraction and how we can use the POSIX API to create multithreaded programs. 

## Introduction

A thread is a sequence of scheduled executions belonging to a process. As such, threads share resources such as memory and files with other threads belonging to the same process. Additionally, they share the same virtual address space as the process they belong to. However, each thread has its own program counter, stack, and set of registers. 

Threads can be user-level or kernel-level with the following differences:

* **User Level:** 

    Implemented in the user-space as a library for user-space programs to use, separate from the OS kernel. A new stack is created for each created thread and the library will keep a queue of runnable threads. It will replace blocking system calls and schedule periodic timer signals. Lightweight and fast, but are limited in resources and not scalable. A blocking system call will block all threads and deadlock is possible.

* **Kernel Level:** 

    Implemented and entirely managed by the OS. It is heavily based of the process abstraction but strips out specific features such as new virtual address spaces and file tables. Heavyweight and slow, but can use all available resources and is scalable.

There is a threading scheme called `n:m` threading where we implement `n` user threads on top of `m` kernel threads, allowing user threads to take advantage of multiple CPU cores. However, they have the same issues as user threads and the kernel will not know relative importance of the threads.

## Multithreaded Programs

Threads allow for multithreaded programs. Multithreading improves the performance of a program by enabling concurrency, which in turn, allows efficient resource usage especially with the CPU. Additionally, it allows programs to respond to user requests more quickly while background computations are happening. We prefer creating threads over forking processes because threads are more lightweight.

Despite its benefits, there are multiple issues that must be considered when dealing with threads. Possibly the greatest issue is **synchronization**, and it is worthy of its own chapter right after threads. However, threads can also:

* Reduce overall performance because of overhead
* Make code much more complex
* Become a bottleneck as the number of threads increases

Let's investigate the POSIX API, which is a multithreading API for the C programming language that provides functions for dealing with user level threads.

## POSIX API Basics

### `int pthread_create(pthread_t *thread, pthread_attr_t *attr, void *(*func) (void *), void *arg))`:

1. **Description**:

    Creates a new thread.

2. **Arguments**:

    - `thread`: A pointer to a `pthread_t` object that will hold the ID of the new thread
    - `attr`: A pointer to a `pthread_attr_t` that specifies the attributes of the new thread. When set to `NULL`, default settings are used
    - `start_routine`: A pointer to the function that the new thread will execute
    - `arg`: An array of arguments that will be passed to the `start_routine` function

3. **Return Values**:

    0 if successful, and some error code otherwise

 


### `void pthread_exit(void *return_value)`:

$\hspace{0.5cm}$ Terminate the calling threads. It only requires one argument, which is an optional exit value for the thread.

 

### `int pthread_join(pthread_t thread, void **return_value)`

1. **Description**:

    Waits for a thread to terminate

2. **Arguments**:

    - `thread`: The `pthread_t` object that holds the ID of the thread to wait for
    - `**value_ptr`: A pointer to a location that will hold the exit value of the thread, or `NULL` if the exit value is not needed.

3. **Return Values**:

    0 if successful, and some error code otherwise

 

### `int pthread_yield()`

1. **Description**:

    Gives up the current thread's usage of the CPU to other threads

2. **Return Values**:

    0 if successful, and some error code otherwise

2. **Other Notes**:

    Although it is not discussed in class, it appears that this particular function is not standardized and only appears on certain systems. The more general function for yielding CPU usage is `sched_yield()`, and `pthread_yield()` function ultimately wraps around it.
















 


# Synchronization

While discussing threads, we briefly mentioned synchronization as a major issue with multithreaded programs. Here, we will get into depth about what this issue actually is and how we can overcome it using locks, another abstraction that the operating system offers.


## Sequential Consistency and Atomicity

Before we closely investigate synchronization, it is a good idea to first understand both sequential consistency and atomicity.

If we need to develop a program that must be correct and consistent, we should strive for sequential consistency, which is the principle that the result of execution is as if all operations were executed in some sequential order, and the operations of each processor occurred in the order specified by the program. If we aim for sequential consistency, we can ensure that our programs are deterministic and predictable even if they are multithreaded (single threaded programs are sequentially consistent by nature). 

For a program to have sequential consistency, we need to maintain program order on individual processors and ensure write atomicity. Since not all CPUs have sequential consistency because it negatively affects optimizations and complicates buffering, re-ordering, reads, and cache coherence, our code should utilize synchronization primitives, memory barriers, atomic operations and thread-safe data structures. wd

The question then becomes, how can we ensure that our multithreaded programs have sequential consistency? How can we make multiple threads work together to achieve the desired output of a program? The answer is correctly synchronizing our threads. Improper synchronization can lead to concurrency bugs that can cause unpredictable behaviour.


## Critical Sections and Data Races

All synchronization problems occur at **critical sections**, which are sections of code that accesses and possibly mutates shared resources such as variables, data structures, and files. If two or more threads try to access a shared resource without proper synchronization, then we have a **data race** that can cause unpredictable behaviour (ie. our program is not sequentially consistent).

Consider the following code sample, where we hope to increment a global variable `counter` with two threads:

```c
#include <pthread.h>
#include <stdio.h>

volatile int counter = 0;

void *increment_counter(void *arg) {
    for (int i = 0; i < 1000000; i++) {
        counter++;
    }
    return NULL;
}

int main() {
    pthread_t thread1, thread2;

    pthread_create(&thread1, NULL, increment_counter, NULL);
    pthread_create(&thread2, NULL, increment_counter, NULL);

    pthread_join(thread1, NULL);
    pthread_join(thread2, NULL);

    printf("Final value of counter: %d\n", counter);
    return 0;
}
```

While this program executes, both threads are accessing and modifying the `counter` varible concurrently. The part where `counter` is accessed and modified in `increment_counter()` is a critical section. Unfortunately, we have a data race present, since it is possible for one thread to read `counter` then the other to read, increment, and update `counter`. The threads can overwrite each other's updates and cause the final value of `counter` to be incorrect, where it should be `2000000`.


## Types of Locks

Mutex: A mutex is a synchronization primitive that is used to protect shared resources from concurrent access. A mutex allows only one thread to enter the critical section of code at a time, ensuring that the shared resource is accessed in a mutually exclusive way. A mutex can be used when there is a need to protect a critical section of code that needs to be accessed in a mutually exclusive way. Mutexes are typically used for short critical sections, as they can be less efficient than other types of locks for longer sections of code.

Conditional Variable: A conditional variable is a synchronization primitive that is used to block a thread until a certain condition is met. A conditional variable allows threads to synchronize on a particular state or event, allowing them to wait until a certain condition is met before continuing. A conditional variable can be used when there is a need for a thread to wait for a certain condition to become true before proceeding with further work. For example, a producer-consumer scenario where the consumer waits for the producer to signal that new data is available.

Semaphore: A semaphore is a synchronization primitive that is used to control access to a shared resource by multiple threads. A semaphore maintains a count of the number of threads that can access the shared resource, allowing a fixed number of threads to access the resource at a time. A semaphore can be used when there is a need to limit the number of threads that can access a shared resource simultaneously, or when coordinating the activity of a fixed number of threads.

Spinlock: A spinlock is a synchronization primitive that is used to protect a shared resource from concurrent access. Unlike a mutex, a spinlock does not put the thread to sleep when the lock is already held by another thread. Instead, it repeatedly polls the lock until it is available, spinning in a loop until it can acquire the lock. A spinlock can be used when the critical section of code is expected to be short and the contention for the lock is expected to be low. Spinlocks are typically used in scenarios where a mutex would introduce too much overhead, such as in kernel code or other low-level system components.

In terms of when to use each type of lock, it depends on the specific scenario and requirements of the application. In general:

Use a mutex when there is a need to protect a critical section of code that needs to be accessed in a mutually exclusive way.
Use a conditional variable when there is a need for a thread to wait for a certain condition to become true before proceeding with further work.
Use a semaphore when there is a need to limit the number of threads that can access a shared resource simultaneously, or when coordinating the activity of a fixed number of threads.
Use a spinlock when the critical section of code is expected to be short and the contention for the lock is expected to be low.


What we want are:

* Mutual Exclusion (only one thread can be in critical section at a time)
* Progress (a process will eventually get in a critical section)
* Bounded waiting (a bound on the number of times other threads get in)

