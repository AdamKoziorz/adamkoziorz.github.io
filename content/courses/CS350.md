---
title: "CS350 - Operating Systems"
url: "courses/CS350/"
summary: An introduction to the fundamentals of operating system function, design, and implementation
ShowToc: true
math: true
---

> **Instructor:** Rob Hackman \
> **Lectures:** T/Th at 10:00am \
> **Section:** 001 \
> \
> **Course Breakdown:** \
> Assignments: 2%, 12%, 11%, 15% \
> Reading Assignments: 2 $\times$ 5% \
> Midterm: 1 $\times$ 20% \
> Final Exam: 1 $\times$ 30% \
> \
> **Course Description:** \
> An introduction to the fundamentals of operating system function, design, and implementation. Topics include concurrency, synchronization, processes, threads, scheduling, memory management, file systems, device management, and security.

 

# Introduction

## Motivation
How do multiple programs run at the same time and utilize computer resources? The "middle man" responsible for this is the Operating System (OS). This is a maturing field and many different computer science topics are OS issues (high-performance servers, resource consumption, security, databases, game engines, etc.)

We will learn the basics of operating systems, what they do, what issues they can face, and how they affect software. Some key concepts include multi-programming, concurrency, memory management, device management, and file systems. We will focus on these concepts with an emphasis on security and protection. 


## What is an Operating System?

The operating system (OS) is a layer between applications and hardware that provides an **abstraction** to the hardware details and program resources, allowing programmers to interact with hardware securely and effectively. It contains a kernel that responds to system calls, interrupts, and exceptions.

Originally, they were just a library of standard services that didn't really offer any security and the system could only run one program at a time. However, 
multitasking capabilities were then realized to make it appear as though multiple programs are running at the same time to allow for concurrency. More features were eventually added, leading to a prototype of a modern OS.

### Views of an OS

1. **Application View**:

    The OS provides an execution environment for running programs. This environment provides us with an interface to access networks, storage, I/O devices, as well as enables programs to run alongside other programs with the appropriate protections.

2. **System View**:

    The OS manages the hardware resources of a computer system. It allocates the required resources for programs to run and controls how they are shared. Such resources includes processors, memory, disks, storage devices, networks, and I/O devices.

3. **Implementation View**:

    The OS is a concurrent, real-time program where it needs to run alongside other programs and must respond to events with specific timing constraints.

### How does the OS protect us?

1. **Preemption**:

    Give applications a resource, take it away if needed elsewhere

2. **Interposition**:

     Place the OS between application and "stuff", track all pieces that application allowed to use, and on every access, look in table to check that access legal

3. **(Un)privileged Modes**:

     Applications are unprivileged and the OS is privileged - system calls can only be done in privileged mode

### System Calls

If we have to perform system calls (unprivileged to privileged), then special instructions will invoke a `syscall` handler which stores the state of the registers, loads into the program counter a predetermined address in the kernel memory (enabling privileged mode), then returns back to the original application (disabling privileged mode).

We want to perform system calls if our application needs to do things that it cannot do in unprivileged mode. Higher-level functions such as `printf`, `gets`, `scanf`, etc. are built upon the `syscall` interface.

### Schematic of the OS

Below, we have a schematic of the OS:

<center>
<img src="/cs350-os-example.png" width="550" height="350" alt="OS Schematic">
</center>

 Note that the user programs here would be unprivileged, and that the program needs to interact with the kernel in order to run in privileged mode and access hardware.








 

# Processes

The OS provides processes, threads, locks, and file I/O to applications. Let's focus on the process abstraction first.

## Introduction

**Process:** An instance of a running program

**Thread:** A sequence of scheduled executions (we will discuss this more later)

The OS keeps track of processes using a data structure which includes execution details such as running threads, address space, open files, and more. Each process has their own unique process ID (PID) and will always have at least one thread (the program entry point).

The OS enables multiple processes to run, increasing CPU utilization and reducing latency. Most programs do not care which other programs are running, but not all programs (we will see how to deal with this later).

Here are the states of a process/thread:

* Running $\rightarrow$ Blocked: Asks for a resource that is not available
* Blocked $\rightarrow$  Ready: Resource becomes available
* Ready $\rightarrow$  Running: When the OS schedules it (core is available)
* Running $\rightarrow$  Ready: Yielding or preemption

<center>
<img src="/cs350-process-states.png" width="550" height="250" alt="Process States">
</center>

Whenever a thread stalls for input, another thread will execute on the CPU core until input has been received (moving the thread into a ready state) AND the other thread has been kicked off by the OS. A nice real-life analogy to this is sharing equipment with someone on an fixed ON/OFF cycle.


## User View of Processes

### `int fork (void)`:

1. **Description**:

    Creates a new process that is a (near) exact copy of the current one

2. **Return Values**:

    - `pid` of the new process in "parent" process after the fork
    - `0` in the "child" process after the fork

3. **Example Usage**:

    ```c
    #include <unistd.h>
    #include <sys/wait.h>

    switch (pid = fork()) {    // we check the return value of fork()
        case -1:
            perror("fork"); break;
        case 0:
            // do something in child process
        default:
            // do something in parent process
    }
    ```

4. **Other Notes**:

    We would like to use forks in cases where we might want to have parallelism (Nginx, PostgreSQL, etc.). It is very simple to deal with as there are no arguments, and we can use the child to perform any different tasks like manipulate file descriptors, environment, and set resource limits. Note that the processes are independent, so they have their own memory.  

 

### `int waitpid (int pid, int *stat, int opt)`:

1. **Description**:

    Suspends execution of the calling process until the process indicated by `pid` has changed state (ends or is stopped)

2. **Arguments**:

    - `pid`: Process to wait for, or -1 for any
    - `stat`: The exit value or signal
    - `opt`: Usually `0` or `WNOHANG`

3. **Return Values**:

    - `pid` if successful, otherwise `-1`

4. **Example Usage**:

    ```c
    #include <unistd.h>
    #include <stdio.h>
    #include <sys/wait.h>

    int main() {
        int rc1, rc2, rc3;
        rc1 = fork();
        rc2 = fork();
        rc3 = fork();
        int exit_status;
        printf("1");                      // This gets printed 8 times!
        waitpid(rc1, &exit_status, 0);
        waitpid(rc2, &exit_status, 0);
        waitpid(rc3, &exit_status, 0);
    }
    ```

5. **Additional Notes**:

    - We use this function if and only if we need to wait for a *child* process to finish execution to continue executing. This should always be used if a process is being forked so all children processes are finished before the parent process can finish.

    - If a process exits but its parent is still alive, then we must store the exit code status somehow. The best way to approach this is to destroy its information but leave the data structures available and leave an exit code.

    - You can only free processes when waitpid is called on it or when its parent exits (you need to check all the children in this case).

    - If process A creates process B and process A exits, then process B should **not** be killed. However, when process B returns, then it needs to check if its parents is alive, and if not, then process B is responsible for cleaning itself up.


 

### `void exit (int status)`:

$\hspace{0.5cm}$ Closes the current process and passes `status` to its parent (usually through `waitpid`) for it to use as it wishes.

 

### `int kill (int pid, int sig)`:

1. **Description**:

    Sends `sig` to a process `pid` to forcibly interrupt that process

2. **Arguments**:

    - `pid`: The pid of the process we want to interrupt
    - `sig`: The signal we wish to send to the process

3. **Return Values**:

    - `0` if successful, otherwise `-1`


 


### `int execve(char *prog, char **argv, char **envp)`:

1. **Description**:

    Executes a specified program (`prog`) and passes the provided command-line arguments and environment variables.

2. **Arguments**:

    - `prog`: Full pathname of program to run
    - `argv`: Argument vector that gets passed to main
    - `envp`: Environment variables (eg. `PATH`, `HOME`)

3. **Return Values**:

    - `execve` should not return any values

4. **Example Usage**:

    ```c
    #include <sys/wait.h>
    #include <errno>

    int main(int argc, char **argv) {
        int rc = fork();
        if (rc == 0) {
            char *args[] = {"grep", argv[1], "alice.txt", NULL};
            execvp("grep", args);
            printf("Execv error code: %d\n", x);
            printf("Execv errno: %d\n", errno);
        }
        int status;
        waitpid(rc, &status, 0);
        if (WEXITSTATUS(status) == 0) {
            printf("Word %s was found \n", argv[1]);
        } else {
            printf("Word %s was not found \n", argv[1]);
        }
    }
    ```

5. **Additional Notes**:

    - This does not create a new process - it changes the identity of the program instead. Therefore, it should only be used on a child process
    - There are variants of this function (`execvp` and `exevlp`) that have similar functionality to `execve` but with some small differences - these will not be covered individually

 

### `int dup2 (int oldfd, int newfd)`:

This command will close `newfd` if it was a valid file descriptor and makes `newfd1` an exact copy of `oldfd`. Two file descriptors will share the same offset.

Note that `dup2(x, 0)` will change `x`'s file descriptor to `stdin` and `dup2(x, 1)` will change `x`'s file descriptor to `stdout`.

### `int fcntl (int fd, F_SETFD, int val)`:

This will set the `close on exec` if `val = 1` and clears if `val = 0`. The file descriptor will be made non-inheritable by the new program.

### `int pipe (int fds[2])`:

This will return two file descriptors in `fds[0]` and `fds[1]`. Any writes to `fds[1]` will be read on `fds[0]`, and when the last copy of `fds[1]` is closed, then `fds[0]` will return `EOF`. It will return `0` on success and `-1` on error.

Pipes can read, write, and close just like with files. If `fds[1]` is closed, then `read(fds[0])` will return 0 bytes. When `fds[0]` is closed, then `write(fds[1])` and we kill the process.


 

## Kernel View of Processes

### Implementation

The OS keeps a data structure for each proc (Process Control Block). It will track the information necessary for each process to run, which includes:
* Process State and Process ID
* Registers and Program Counter
* Address Space
* Open Files

If a process is forked, the parent and child processes will share the same address space, but they will not share memory.

### User and Kernel Modes

A key component of an operating system is to introduce restrictions on what a process can do for safety purposes (I/O, device connection, create new processes, etc.). The way the OS does this is to have a **user mode** (unprivileged) and a **kernel mode** (privileged).

Because our own programs may need to execute privileged instructions, the kernel will provide the ability to perform system calls.

How this works is that the program will execute a `trap` instruction which sends a signal to the OS to switch to kernel mode and executes the appropriate kernel code.  Then once this kernel code is executed, the OS calls a `return-from-trap` instruction that returns to user mode and continues executing the original program.

To ensure that the proper code is executed, the kernel needs to set up a trap table that tells the hardware what code to run when certain signals are received.

<center>
<img src="/cs350-trap-example.png" width="500" height="600" alt="Trap Example">
</center>





### Preemption

We can preempt (interrupt) a process when the kernel gets control. If there is such an interrupt (scheduling quantum, device recognition, etc.), then we need to save the state of the process/thread before the kernel can run its code. This is what is known as context-switching. We can use a switch statement to understand the content of the interrupt, of which the appropriate kernel code will be executed.
\
The CPU has a clock of which we can set up a timer to perform interrupts.

Whenever a process is interrupted, it will save:
1. The program counter and integer registers (always)
2. Special registers and floating points
3. Condition codes

<center>
<img src="/cs350-switch-example.png" width="500" height="500" alt="Switch Example">
</center>


then possibly change virtual address translations. We can then see that context switching is expensive and can cause TLB flushes as well as cache misses.

In OS161, if process 1 gets preempted, then the following calls occur on its stack:

Thread data $\to$ Trap Frame $\to$ MIPS trap (general) $\to$ mainbus_interrupt (specific) $\to$ thread_switch $\to$ switchframe_switch

Switch frames are just the special registers, and when we call `switchframe_switch`, its returns into thread_switch in process 2's stack (we switch threads in the middle of the execution of switchframe_switch). Implementation is found in `switch.S`.



### Scheduling

* If 0 threads are runnable, then halt CPU
* If 1 thread is runnable, then run the thread
* If 2+ threads are runnable, make a scheduling decision.  We could scan the process table, use FIFO/Round-Robin (this is used in OS161) or use priorities.

To demonstrate FIFO, consider thread $A$, $B$, and $C$ (in this order) and suppose $A$ gets to run until it is finished. According to this system, $B$ then gets to run until it is finished, then $C$ gets to run until it is finished.

Round-Robin is just FIFO but with preemption introduced. So, $A$ runs for a bit then is kicked out. Then, $A$ is placed at the end of the queue and $B$ runs for a bit. Then, $B$ is kicked out, placed at the end of the queue, and $C$ runs for a bit. Now, when $C$ is kicked off, we will run $A$.



















 

# Threads

Now, we will focus on the thread abstraction and how we can use the POSIX API to create multithreaded programs. 

## Introduction

A thread is a sequence of scheduled executions belonging to a process. As such, threads share resources such as memory and files with other threads belonging to the same process. Additionally, they share the same virtual address space as the process they belong to. However, each thread has its own program counter, stack, and set of registers. 

Threads can be user-level or kernel-level with the following differences:

* **User Level:** 

    Implemented in the user-space as a library for user-space programs to use, separate from the OS kernel. A new stack is created for each created thread and the library will keep a queue of runnable threads. It will replace blocking system calls and schedule periodic timer signals. Lightweight and fast, but are limited in resources and not scalable. A blocking system call will block all threads and deadlock is possible.

* **Kernel Level:** 

    Implemented and entirely managed by the OS. It is heavily based of the process abstraction but strips out specific features such as new virtual address spaces and file tables. Heavyweight and slow, but can use all available resources and is scalable.

There is a threading scheme called `n:m` threading where we implement `n` user threads on top of `m` kernel threads, allowing user threads to take advantage of multiple CPU cores. However, they have the same issues as user threads and the kernel will not know relative importance of the threads.

## Multithreaded Programs

Threads allow for multithreaded programs. Multithreading improves the performance of a program by enabling concurrency, which in turn, allows efficient resource usage especially with the CPU. Additionally, it allows programs to respond to user requests more quickly while background computations are happening. We prefer creating threads over forking processes because threads are more lightweight.

Despite its benefits, there are multiple issues that must be considered when dealing with threads. Possibly the greatest issue is **synchronization**, and it is worthy of its own chapter right after threads. However, threads can also:

* Reduce overall performance because of overhead
* Make code much more complex
* Become a bottleneck as the number of threads increases

Let's investigate the POSIX API, which is a multithreading API for the C programming language that provides functions for dealing with user level threads.

## POSIX API Basics

### `int pthread_create(pthread_t *thread, pthread_attr_t *attr, void *(*func) (void *), void *arg))`:

1. **Description**:

    Creates a new thread.

2. **Arguments**:

    - `thread`: A pointer to a `pthread_t` object that will hold the ID of the new thread
    - `attr`: A pointer to a `pthread_attr_t` that specifies the attributes of the new thread. When set to `NULL`, default settings are used
    - `start_routine`: A pointer to the function that the new thread will execute
    - `arg`: An array of arguments that will be passed to the `start_routine` function

3. **Return Values**:

    0 if successful, and some error code otherwise

 


### `void pthread_exit(void *return_value)`:

$\hspace{0.5cm}$ Terminate the calling threads. It only requires one argument, which is an optional exit value for the thread.

 

### `int pthread_join(pthread_t thread, void **return_value)`

1. **Description**:

    Waits for a thread to terminate

2. **Arguments**:

    - `thread`: The `pthread_t` object that holds the ID of the thread to wait for
    - `**value_ptr`: A pointer to a location that will hold the exit value of the thread, or `NULL` if the exit value is not needed.

3. **Return Values**:

    0 if successful, and some error code otherwise

 

### `int pthread_yield()`

1. **Description**:

    Gives up the current thread's usage of the CPU to other threads

2. **Return Values**:

    0 if successful, and some error code otherwise

2. **Other Notes**:

    Although it is not discussed in class, it appears that this particular function is not standardized and only appears on certain systems. The more general function for yielding CPU usage is `sched_yield()`, and `pthread_yield()` function ultimately wraps around it.
















 


# Synchronization

While discussing threads, we briefly mentioned synchronization as a major issue with multithreaded programs. Here, we will get into depth about what this issue actually is and how we can overcome it using locks, another abstraction.


## Sequential Consistency

Before we closely investigate synchronization, it is a good idea to first understand what sequential consistency means.

If we need to develop a program that must be correct and consistent, we should aim for sequential consistency, which is the principle that the result of execution is as if all operations were executed in some sequential order, and the operations of each processor occurred in the order specified by the program. If we achieve sequential consistency and write correct programs, we can ensure that our programs are deterministic and predictable even with multithreading.

For a program to have sequential consistency, we need to maintain program order on individual processors and ensure write atomicity. Not all CPUs have sequential consistency because it negatively affects optimizations and complicates buffering, re-ordering, reads, and cache coherence.

> Atomic operations allow for operations on a shared memory location in a single, indivisible step. This ensures that assembly instructions that mutates data cannot be interrupted by other assembly instructions. We will look into C11 atomic operations a little more in depth shortly.

We will now investigate many synchronization issues that multithreaded programs can face, and how we can fix them using locks and other language-specific features. By correctly synchronizing our threads and utilizing other features, we can write high-level code that behave indistinguishably from sequential consistency.


## Critical Sections and Data Races

Many synchronization problems occur at code that accesses and possibly mutates shared resources such as variables, data structures, and files. If two or more threads try to access a shared resource without proper synchronization, then we have a **data race** that can cause unpredictable behaviour (ie. our program is not sequentially consistent).

Consider the following code sample, where we hope to increment a global variable `counter` with two threads:

```c
#include <pthread.h>
#include <stdio.h>

volatile int counter = 0;

void *increment_counter(void *arg) {
    for (int i = 0; i < 1000000; i++) {
        counter++;
    }
    return NULL;
}

int main() {
    pthread_t thread1, thread2;

    pthread_create(&thread1, NULL, increment_counter, NULL);
    pthread_create(&thread2, NULL, increment_counter, NULL);

    pthread_join(thread1, NULL);
    pthread_join(thread2, NULL);

    printf("Final value of counter: %d\n", counter);
    return 0;
}
```

While this program executes, both threads are accessing and modifying the `counter` varible concurrently. The part where `counter` is accessed and modified in `increment_counter()` is a critical section. Unfortunately, we have a data race present, since it is possible for one thread to read `counter` then the other to read, increment, and update `counter`. The threads can overwrite each other's updates and cause the final value of `counter` to be incorrect, where it should be `2000000`.

To resolve this, we need to establish a **critical section**, which is a section of code that cannot be executed concurrently. Ideally, our implementation for critical sections satisifies the following conditions:

1. **Mutual exclusion** (one thread in a critical section at all times)
2. **Progress** (all processes trying to enter a critical section will get in)
3. **Bounded waiting** (there is a bound for how many times other threads get in)

The mechanisms that protect critical sections are known as synchronization primitives. Mutexes, a type of synchronization primitive, can protect the `counter` variable and ensure that the program always outputs `2000000`:

```c
#include <pthread.h>
#include <stdio.h>

volatile int counter = 0;
pthread_mutex_t mutex = MUTEX_INITIALIZER;

void *increment_counter(void *arg) {
    for (int i = 0; i < 10000; i++) {
        pthread_mutex_lock(&mutex);     // enter critical section
        counter++; 
        pthread_mutex_unlock(&mutex);   // exit critical section
    }
    return NULL;
}

int main() {
    pthread_t thread1, thread2;

    pthread_mutex_init(&mutex, NULL);

    pthread_create(&thread1, NULL, increment_counter, NULL);
    pthread_create(&thread2, NULL, increment_counter, NULL);

    pthread_join(thread1, NULL);
    pthread_join(thread2, NULL);

    printf("Final value of counter: %d\n", counter);

    pthread_mutex_destroy(&mutex);
    return 0;
}
```


## Synchronization Primitives

### Mutexes

Mutexes are used to protect shared resources from concurrent access. A mutex allows only one thread to enter the critical section of code at a time, ensuring that the shared resource is accessed in a mutually exclusive way. A mutex should be used when there is a need to protect a _short_ critical section of code, especially when global variables are mutated.

The POSIX API provides us with mutexes:

* **`int pthread_mutex_init(pthread_mutex_t *m, pthread_mutexattr_t attr)`:**

    Initializes a mutex with a mutex variable and mutex attributes. It is unlocked by default.

* **`int pthread_mutex_destroy(pthread_mutex_t *m)`:**

    Destroys the mutex specified by the mutex variable pointer.

* **`int pthread_mutex_lock(pthread_mutex_t *m)`:**

    Lets the current thread acquire the mutex specified by the mutex variable pointer, of which it will enter the critical section and block other threads from accessing.

* **`int pthread_mutex_unlock(pthread_mutex_t *m)`:**

    Lets the current thread release the mutex specified by the mutex variable pointer, of which it will exit the critical section and allow other threads to access it.


### Conditional Variables

Conditional variables are used to block a thread until a certain condition is met. A conditional variable allows threads to synchronize on a particular state or event, allowing them to wait until a certain condition is met before continuing. A conditional variable should be used when there is a need for a thread to wait for a certain condition to become true before proceeding with further work.

The POSIX API provides us with conditional variables:

* **`int pthread_cond_init(pthread_cond_t *c, pthread_condattr_t *attr)`:**

    Initializes a conditional variable with a conditional variable object and its attributes.

* **`int pthread_cond_destroy(pthread_cond_t *m)`:**

    Destroys the conditional variable specified by the conditional variable pointer.

* **`int pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m)`:**

    Unlocks `m` for the current thread and sleeps. If `c` is signaled, then `m` will become locked and resume execution.

* **`int pthread_cond_signal(pthread_cond_t *c)`:**

    Signals _one_ thread that the condition it is waiting for has become true.

* **`int pthread_cond_broadcast(pthread_cond_t *c)`:**

    Signals _all_ threads that the condition it is waiting for has become true.

We will see an application of conditional variables shortly.

### Semaphores

Semaphores are used to control access to a shared resource by multiple threads. A semaphore maintains a count of the number of threads that can access the shared resource, allowing a fixed number of threads to access the resource at a time. A semaphore should be used when there is a need to limit the number of threads that can access a shared resource simultaneously or when coordinating the activity of a fixed number of threads.

The POSIX API provides us with semaphores:

* **`int sem_init(sem_t *sem, int pshared, unsigned int val)`:**

    Initializes a semaphore with a semaphore variable, a int (implemented as a boolean) that indicates if the semaphore should be shared between processes (`pshared = 1`) or threads of the same process (`pshared = 0`), and an int representing the initial value of the semaphore.

* **`int sem_destroy(sem_t *sem)`:**

    Destroys the semaphore specified by the semaphore variable pointer.

* **`int sem_wait(sem_t *sem)`:**

    Decrements the value of the semaphore by one. If the value of the semaphore is already zero, then the calling thread is blocked and added to a queue until the semaphore value is incremented somewhere else (possibly by a call to `sem_post`) and it is at the front of the queue.

* **`int sem_post(sem_t *sem)`:**

    Increments the value of the semaphore by one if the value is not maximized, and does nothing if it is maximized.


### Summary

In terms of when to use each type of lock, it depends on the specific scenario and requirements of the application. In general:

* Use a mutex when there is a need to protect a critical section of code that needs to be accessed in a mutually exclusive way.
* Use a conditional variable when there is a need for a thread to wait for a certain condition to become true before proceeding with further work.
* Use a semaphore when there is a need to limit the number of threads that can access a shared resource simultaneously, or when coordinating the activity of a fixed number of threads.

There are other synchronization primitives that exist, but these are the main ones. Spinlocks are another synchronization primitive that will be briefly discussed in the **Atomic Instructions** section rather than this section due to its usage in low-level code to create pseudo-atomic instructions.

## Producer-Consumer Problem

The producer-consumer problem is a synchronization problem in which we have two concurrent threads where

1. One thread produces data and add them to a buffer
2. One thread consumes data and removes them from a buffer

and we wish to coordinate their activity such that a stall occurs when the producer cannot add more data and the consumer cannot consume more data. Without proper synchronization, we could have data corruption.

We will show how this problem can be solved using each type of synchronization primitive.

### Mutex Solution
```c
#define BUFFER_SIZE = 5

int buffer[BUFFER_SIZE];
int count = 0;
int in = 0;
int out = 0;
mutex_t mutex = MUTEX_INITIALIZER

void producer() {
    while (1) {
        item *next = produce_item();
        mutex_lock(&mutex)
        while (count == BUFFER_SIZE) {
            mutex_unlock(&mutex);
            thread_yield();
            mutex_lock(&mutex);
        }
        mutex_unlock(&mutex);
    }
    buffer[in] = next;
    in = (in + 1) % BUFFER_SIZE;
    count++;
    mutex_unlock(&mutex);
}

void consumer() {
    while (1) {
        mutex_lock(&mutex)
        while (count == 0) {
            mutex_unlock(&mutex);
            thread_yield();
            mutex_lock(&mutex);
        }
        mutex_unlock(&mutex);
    }
    item *next = buffer[out]
    out = (out + 1) % BUFFER_SIZE;
    count--;
    mutex_unlock(&mutex);
    consume_item(next);
}
```

### Conditional Variable Solution
```c
#define BUFFER_SIZE = 5; 

int buffer[BUFFER_SIZE];
int count = 0;
int in = 0;
int out = 0;
mutex_t mutex = MUTEX_INITIALIZER;
cond_t nonempty = COND_INITIALIZER;
cond_t nonfull = COND_INITIALIZER;

void producer () {
    while (1) {
        item *next = produce_item();
        mutex_lock(&mutex);
        while (count == BUFFER_SIZE) {
            cond_wait(&nonfull, &mutex);
        }
        buffer[in] = next;
        in = (in + 1) % BUFFER_SIZE;
        count++;
        cond_signal(&nonempty);
        mutex_unlock(&mutex);
    }
}

void consumer () {
    while (1) {
        mutex_lock (&mutex);
        while (count == 0) {
            cond_wait(&nonempty, &mutex);
        }  
        item *next = buffer[out];
        out = (out + 1) % BUFFER_SIZE;
        count--;
        cond_signal(&nonfull);
        mutex_unlock(&mutex);
        consume_item(next);
    }
}
```

### Semaphore Solution
```c
#define BUFFER_SIZE = 5; 

int buffer[BUFFER_SIZE];
int count = 0;
int in = 0;
int out = 0;
sem_t = empty;           // Initialized to N
sem_t = full;            // Initialized to 0
pthread_mutex_t mutex;

void producer() {
    while (1) {
        item *next = produce_item();
        sem_wait(&empty);
        buffer[in] = next;
        in = (in + 1) % BUFFER_SIZE;
        sem_signal(&full);
    }
}

void consumer() {
    while (1) {
        sem_wait(&full);
        item *next = buffer[out];
        out = (out + 1) % BUFFER_SIZE;
        sem_signal(&empty);
        consume_item(next);
    }
}
```

## Additional Topics

### Spinlocks

In the case where the expected waiting time for a lock is very short, a synchronization primitive called spinlocks may be favourable as they avoid the overhead of context switching and thread blocking. They work by repeatedly trying to acquire the lock in a loop without yielding the processor until the lock is actually acquired. They are usually implemented using low-level assembly code or processor-specific instructions like `xchg`.

### C11 Atomics

Recall that atomic instructions are ideal since they can prevent race conditions and other concurrency problems. The C11 standard provides an atomic type `_Atomic(type)` and atomic operations for multithreaded programming across many platforms. We are also able to control the memory model (`memory_order_relaxed` to `memory_order_seq_cst`) for each atomic operation we perform. Using C11 atomics, we can actually replace the need for synchronization primitives altogether, though it may not be ideal in cases where we require greater flexibility.


### Cache Coherence

When we choose to cache information for performance, we risk having processors disagree with the memory. We can think of cache coherence as the processors having a consistent view of shared memory data.

The most common cache coherency protocol is the MSI protocol. Here, the cache is divided into chunks of bytes called cache lines which can have the following states:

* Modified (present in one cache and has been modified, and it must be written back to memory or another cache before it can be shared again)
* Shared (present in multiple caches and all copies are identical - if modified, then all other duplicates must become invalidated and this specific cache line will be in the modified state)
* Invalid (not accessible and contains no data)

Then, the MSI protocol carefully defines how the cache transitions between these states, thus ensuring cache coherence. Some important implications of this:

1. Avoid false sharing (data from different threads should not be in the same cache line)
2. Align structures to cache lines (related data should be placed together)
3. Pad data structures (unused fields will ensure alignment)
4. Avoid contending on cache lines (reduce traffic)

### Deadlock

Deadlock is a situation in concurrent programming where two or more processes are blocked, each waiting for the other to release a resource that they need in order to proceed. Whenever we acquire locks in different orders or hold locks while crossing abstraction barriers, we risk entering a deadlock.

The following four conditions are all required for deadlock to occur:

1. Limited access (resource can only be shared with finite processes)
2. No preemption (once resource granted, cannot be taken away)
3. Hold and wait (wait for next resource while holding current one)
4. Circularlity in graph of requests (the graph of the system contains a cycle)

We can choose to either prevent deadlock by eliminating one of these conditions or correct deadlock whenever it occurs.
































 


# System Calls

While talking about processes, we discussed how we could use system calls such as `fork()` and `waitpid()` to perform privileged tasks. They can be thought of as the kernel's application programmer interface (API) which programmers can use to interact with the operating system.

 It is worth talking about system calls as well as interrupts and exceptions in more detail as it pertains to the hardware and operating system.

 ## Crossing into Kernel Mode

 As discussed in the processes section, computer hardware provides both a kernel mode (privileged) and user mode (unprivileged). Kernel mode can access privileged CPU features that user mode can't, which allows the kernel to protect itself and isolate processes.

 There are two ways we can enter kernel mode from user mode:

 1. **Interrupts:**

    Interrupts are requests _raised_ by devices. The device will send a signal to the processor through a physical pin or bus message, from which the processor will interrupt the current program and begin executing the interrupt handler in kernel mode. Most interrupts can be disabled, but not all.

2. **Exceptions:**

    Exceptions are conditions _encountered_ by the processor itself while executing a program. Program errors, operating system requests, and hardware errors are all examples of exceptions. The processor handles exceptions very similarly to interrupts, but stops the current program execution at the precise instruction that triggered the exception and uses an exception handler.

    There are many different exceptions in MIPS, the most important one being `EX_SYS`, the syscall exception. When this exception is triggered, the application loads the arguments into CPU registers (with `$v0` being the system call number) and executes the syscall instruction. Then, the kernel processes the system call through the exception handler and then use `rfe` to return to userspace.

So ultimately, the following steps occur whenever we wish to perform a syscall:

1. Application calls the C library
2. Library executes the syscall instruction
3. Kernel exception handler runs
    
    * Switches to kernel stack

    * Creates a trapframe containing the program state

    * Determine the type of exception
    
    * Determine the type of system call

    * Run the function in the kernel

    * Restore application state from the trap frame

    * Return from application using `rfe`

4. Library wrapper function returns to the application


## Registers and MIPS Conventions

The processor contains registers that hold information. The application binary interface (ABI) defines the contract between applications and system calls. This contract must be obeyed by operating systems and compilers so that the kernel can correctly interpret information and return the correct result.

In addition to system call numbers, we have the following register conventions for MIPS:

Caller-Saved:
* `$t0-$t9`: Temporary registers
* `$a0-$a3`: Argument registers
* `$v0-$v1`: Return values

    - The system call number is stored in `$v0`, which then becomes the return value
    - `$a3` stores whether or not a call is successful
    - All other arguments are pushed onto the stack

Callee-Saved:
* `$s0-$s7`: Saved registers
* `$ra`: Return Address

Instructions:
* `jal`: Jump and link (call function and save return address in `$ra`)
* `jr $ra`: Jump register (return from function)

For more information about the stack and how these registers are saved, refer to the CS241 course content. These conventions are simply brought up to understand the upcoming content.


## System Call Stacks

We will now take a look at what happens to both the user and kernel stacks when a syscall is performed in regards to execution contexts and context switching. Before doing this, however, we will define some terminology:

* **Stack:** A stack (data structure) made up of frames containing locals, arguments, and spilled registers
* **Execution Context:** The environment where functions execute including their arguments, local variables, and memory
* **Context Switch:** Storing the state of a process so it can be restored later on, then restoring a different, previously stored state.

Let's look at how the user stack and kernel stack evolves as we begin executing a program that performs a syscall to write text to the console.

1. Before encountering the syscall, the **user** stack contains the `_start`, `main()`, `printf()`, and `write()` frames in this order (`write()` is at the top of the stack)
2. The `write()` call triggers an exception. We switch to kernel mode and save the execution context in a trapframe named `common_exception`. This trapframe is pushed to the front of the (empty) **kernel** stack.
3. The exception handler begins handling the exception. The **kernel** stack now contains `mips_trap()`, `syscall()`, `sys_write()`, and the console driver frames in this order (the console driver is at the top of the stack)
4. The console driver writes text to console, and all frames in the **kernel** stack (up until the `common_exception` trapframe) is popped. Then, we will restore the CPU state from this trapframe and switch to user mode.
5. `write()` is popped from the **user** stack, and the rest of the program proceeds.


## Preemption Stacks

Lastly, we will take a look at what happens when a process gets preempted due to a timer or device interrupt, and another process begins to run. As defined prior, this is what's called context switching. It is machine dependent and has non-negligible cost due to TLB flushes, cache misses, and floating point registers.

Let's look at how the user stack and kernel stacks evolves as a process is interrupted by a timer and is preempted:

1. A process with a **user** stack is interrupted by a timer so we switch to kernel mode and save the execution context in a trapframe named `common_exception`. This trapframe is pushed to the front of an (empty) **kernel 1** stack.
2. The **kernel 1** stack contains a `mips_trap()` frame, and this function notices an `EX_IRQ` exception from the timer. So then, we add `mainbus_interrupt`, `timer_interrupt`, `thread_yield`, `thread_switch` and a `switchframe` to the **kernel 1** stack. Switching processes is a switch between kernel threads, and so we need to store the kernel context in a switchframe.
3. A secondary, **kernel 2** stack identical to **kernel 1** then pops its contents up until the `common_exception` trapframe.
